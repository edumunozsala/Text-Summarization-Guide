{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Guide \n",
    "# Training an Encoder Decoder with Bahdanau Attention and W&B logging\n",
    "\n",
    "In this notebook we will describe the most relevant steps to start training a custom algorithm in AWS SageMaker, not using a custom container, showing how to deal with experiments and solving some of the problems when facing with custom models when using SageMaker script mode on. Some basics concepts on SageMaker will not be detailed in order to focus on the relevant concepts.\n",
    "\n",
    "Following steps will be explained: \n",
    " \n",
    "1. Create an Experiment and Trial to keep track of our experiments\n",
    "\n",
    "2. Load the training data to our training instance\n",
    "\n",
    "3. Create the scripts to train our custom model, a Transformer.\n",
    "\n",
    "4. Create an Estimator to train our model in a Tensorflow 2.1 container in script mode\n",
    "\n",
    "5. Create metric definitions to keep track of them in SageMaker\n",
    "\n",
    "4. Download the trained model to make predictions\n",
    "\n",
    "5. Resume training using the latest checkpoint from a previous training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "For this project we will develope notebooks and scripts to train a Transformer Tensorflow 2 model to solve a neural machine translation problem, traslating simple sentences from English to Spanish. This problem and the model is extensively described in my Mdeium post [\"Attention is all you need: Discovering the Transformer paper\"](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "\n",
    "For this exercise, we’ll use pairs of simple sentences. The source text will be in English, and the target text will be in Spanish, from the Tatoeba project where people contribute, adding translations every day. This is the [link](http://www.manythings.org/anki/) to some translations in different languages. There you can download the Spanish/English `spa_eng.zip` file; it contains 124,457 pairs of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment\n",
    "\n",
    "Let's start by setting up the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import and load the libraries to use in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "# Create a SageMaker session to work with\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Get the role of our user and the region\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print(role)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variables for data locations\n",
    "data_folder_name='data'\n",
    "train_filename = 'cl_Inshorts.csv'\n",
    "# Set the directories for our nodel output\n",
    "trainedmodel_path = 'trained_model'\n",
    "output_data_path = 'output_data'\n",
    "# Set the name of the artifacts that our model generate (model not included) \n",
    "model_info_file = 'model_info.pth'\n",
    "input_vocab_file = 'in_vocab.pkl'\n",
    "output_vocab_file = 'out_vocab.pkl'\n",
    "# Set the absolute path of the train data \n",
    "train_file = os.path.abspath(os.path.join(data_folder_name, train_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with Amazon SageMaker training jobs that will run on containers in a new instance or \"vm\", the data has to be share using a S3 Storage folder. For this purpose we define the bucket name and the folder names where our inputs and outputs will be stored. In our case we define:\n",
    "- The **training data** URI: where our input data is located\n",
    "- The **output folder**: where our training saves the outputs fron our model\n",
    "- The **checkpoint folder**: where our model uploads the checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket_name = 'edumunozsala-ml-sagemaker'\n",
    "# Set the training data folder in S3\n",
    "training_folder = r'ts-enc-dec-attention/train'\n",
    "# Set the output folder in S3\n",
    "output_folder = r'ts-enc-dec-attention'\n",
    "# Set the checkpoint in S3 folder for our model \n",
    "ckpt_folder = r'ts-enc-dec-attention/ckpt'\n",
    "\n",
    "training_data_uri = r's3://' + bucket_name + r'/' + training_folder\n",
    "output_data_uri = r's3://' + bucket_name + r'/' + output_folder\n",
    "ckpt_data_uri = r's3://' + bucket_name + r'/' + ckpt_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/train',\n",
       " 's3://edumunozsala-ml-sagemaker/ts-enc-dec-attention',\n",
       " 's3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ckpt')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_uri,output_data_uri,ckpt_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can upload to the training data folder in S3 the files necessary for training: training data, non breaking prefixes for the inputs (English) and the non breaking prefixes for the outputs (Spanish). Once uploaded they can be loaded for training in the SageMaker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an experiment and trial\n",
    "\n",
    "*Amazon SageMaker Experiments* is a capability of Amazon SageMaker that lets you organize, track, compare, and evaluate your machine learning experiments.\n",
    "\n",
    "Machine learning is an iterative process. You need to experiment with multiple combinations of data, algorithm and parameters, all the while observing the impact of incremental changes on model accuracy. Over time this iterative experimentation can result in thousands of model training runs and model versions. This makes it hard to track the best performing models and their input configurations. It’s also difficult to compare active experiments with past experiments to identify opportunities for further incremental improvements.\n",
    "\n",
    "Experiments will help us to organize and manage all executions, metrics and results of a ML project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.1.24)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker-experiments) (1.16.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (1.19.9)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.9->boto3>=1.12.8->sagemaker-experiments) (1.25.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.9->boto3>=1.12.8->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.9->boto3>=1.12.8->sagemaker-experiments) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library necessary to handle experiments\n",
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries to handle experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries to work with Experiments in SageMaker\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the experiment and trial name and one tag to help us to identify the reason for this items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name='ts-enc-dec-attention'\n",
    "# Set the trial name \n",
    "trial_name=\"{}-{}\".format(experiment_name,'single-gpu')\n",
    "\n",
    "tags = [{'Key': 'my-experiments', 'Value': 'ts-enc-dec-attention'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create an experiment to track all the model training iterations. Experiments are a great way to organize your data science work. You can create experiments to organize all your model development work for : a business use case you are addressing (e.g. create experiment named “customer churn prediction”), or a data science team that owns the experiment (e.g. create experiment named “marketing analytics experiment”), or a specific data science and ML project. Think of it as a “folder” for organizing your “files”.\n",
    "\n",
    "We will create a Trial to track each training job run. But this is just a simple example, not intented to explore all the capabilities of the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment  ts-enc-dec-attention\n",
      "Loaded trial  ts-enc-dec-attention-single-gpu\n"
     ]
    }
   ],
   "source": [
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    training_experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print('Loaded experiment ',experiment_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        training_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                      description = \"Experiment to track trainings on my tensorflow Transformer Eng-Spa\", \n",
    "                                      tags = tags)\n",
    "        print('Created experiment ',experiment_name)\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    single_gpu_trial = Trial.load(trial_name=trial_name)\n",
    "    print('Loaded trial ',trial_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        single_gpu_trial = Trial.create(experiment_name=experiment_name, \n",
    "                             trial_name= trial_name,\n",
    "                             tags = tags)\n",
    "        print('Created trial ',trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trackers\n",
    "\n",
    "Another interesting tool to mention, is Tracker objects. They can store information about different types of topics or objects in our model or training process like inputs, parameters, artifacts or metrics. The tracker is attached to a trial, associating the object to the training job. We can record that information and analyze it later on the experiment. **Note** that only parameters, input artifacts, and output artifacts are saved to SageMaker. Metrics are saved to file.\n",
    "\n",
    "As an example, we create a Tracker to register the input data and two parameters about how that data is processed in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Tracker  TextPreprocessing\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.tracker import Tracker\n",
    "# Create the tracker for the inout data\n",
    "tracker_name='TextPreprocessing'\n",
    "trial_comp_name = None # Change to a an exsting TrialComponent to load it\n",
    "\n",
    "try:\n",
    "    tracker = Tracker.load(trial_component_name=trial_comp_name)\n",
    "    print('Loaded Tracker ',tracker_name)\n",
    "except Exception as ex:\n",
    "    tracker = Tracker.create(display_name=tracker_name)\n",
    "    tracker.log_input(name=\"Text Summarization\", media_type=\"s3/uri\", value=inputs)\n",
    "    tracker.log_parameters({\n",
    "        \"Tokenizer\": 'Word',\n",
    "        \"Text Max Length\": 60,\n",
    "        \"Summ Max Length\": 15,\n",
    "    })\n",
    "    print('Created Tracker ',tracker_name)\n",
    "    \n",
    "# Atach the Tracker to the trial\n",
    "single_gpu_trial.add_trial_component(tracker.trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step consist in create the experiment configuration, a dictionary that contains the experiment name, the trial name and the trial component and it will be used to label our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration definition for our experiment and trial\n",
    "trial_comp_name = 'single-gpu-components'\n",
    "# Set the configuration parameters for the experiment\n",
    "experiment_config = {'ExperimentName': training_experiment.experiment_name, \n",
    "                       'TrialName': single_gpu_trial.trial_name,\n",
    "                       'TrialComponentDisplayName': trial_comp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and show information about the experiment and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:  ts-enc-dec-attention\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-12-01-173244-kpnc',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-12-01-173244-kpnc',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 12, 1, 17, 32, 44, 322000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 12, 1, 17, 32, 44, 322000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, secondary status: Completed, failure reason: .'),creation_time=datetime.datetime(2020, 12, 1, 10, 16, 43, 293000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 12, 1, 10, 33, 1, 944000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-12-01-09-57-53-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-12-01-09-57-53-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-12-01-09-57-53', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, secondary status: Completed, failure reason: .'),creation_time=datetime.datetime(2020, 12, 1, 9, 58, 6, 517000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 12, 1, 10, 6, 45, 867000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-12-01-095443-ezlt',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-12-01-095443-ezlt',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 12, 1, 9, 54, 43, 775000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 12, 1, 9, 54, 43, 775000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-11-29-191016-woqj',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-11-29-191016-woqj',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 11, 29, 19, 10, 16, 374000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 19, 10, 16, 374000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-29-18-29-44-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-29-18-29-44-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-29-18-29-44', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Failed',message='Status: Failed, secondary status: Failed, failure reason: AlgorithmError: ExecuteUserScriptError:\\nCommand \"/usr/bin/python3 train.py --att_units 128 --epochs 3 --gru_units 1024 --model_dir s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-11-29-18-29-44/model --nsamples 20000 --resume False --train_file cl_Inshorts.csv --vocab_size 16384\".'),creation_time=datetime.datetime(2020, 11, 29, 18, 29, 47, 537000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 18, 34, 58, 235000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-29-18-16-25-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-29-18-16-25-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-29-18-16-25', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Failed',message='Status: Failed, secondary status: Failed, failure reason: AlgorithmError: ExecuteUserScriptError:\\nCommand \"/usr/bin/python3 train.py --att_units 128 --epochs 3 --gru_units 1024 --model_dir s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-11-29-18-16-25/model --nsamples 20000 --resume False --train_file cl_Inshorts.csv --vocab_size 16384\".'),creation_time=datetime.datetime(2020, 11, 29, 18, 16, 30, 851000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 18, 22, 2, 230000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-29-18-04-58-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-29-18-04-58-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-29-18-04-58', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Failed',message='Status: Failed, secondary status: Failed, failure reason: AlgorithmError: ExecuteUserScriptError:\\nCommand \"/usr/bin/python3 train.py --att_units 128 --epochs 3 --gru_units 1024 --model_dir s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-11-29-18-04-58/model --nsamples 50000 --resume False --train_file cl_Inshorts.csv --vocab_size 16384\".'),creation_time=datetime.datetime(2020, 11, 29, 18, 5, 1, 782000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 18, 10, 13, 745000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-11-29-175149-xsne',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-11-29-175149-xsne',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 11, 29, 17, 51, 49, 403000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 17, 51, 49, 403000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-29-17-02-23-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-29-17-02-23-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-29-17-02-23', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, secondary status: Completed, failure reason: .'),creation_time=datetime.datetime(2020, 11, 29, 17, 2, 26, 539000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 17, 28, 40, 130000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-29-16-53-39-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-29-16-53-39-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-29-16-53-39', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Failed',message='Status: Failed, secondary status: Failed, failure reason: AlgorithmError: ExecuteUserScriptError:\\nCommand \"/usr/bin/python3 train.py --att_units 128 --epochs 3 --gru_units 1024 --model_dir s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-11-29-16-53-39/model --nsamples 50000 --resume False --train_file cl_Inshorts.csv --vocab_size 16384\".'),creation_time=datetime.datetime(2020, 11, 29, 16, 53, 58, 749000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 16, 59, 12, 928000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-29-16-42-06-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-29-16-42-06-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-29-16-42-06', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, secondary status: Completed, failure reason: .'),creation_time=datetime.datetime(2020, 11, 29, 16, 42, 10, 322000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 16, 49, 19, 41000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-11-29-164102-oblw',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-11-29-164102-oblw',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 11, 29, 16, 41, 2, 366000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 29, 16, 41, 2, 366000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-28-18-11-07-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-28-18-11-07-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-28-18-11-07', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Stopped',message='Status: Stopped, secondary status: Stopped, failure reason: .'),creation_time=datetime.datetime(2020, 11, 28, 18, 11, 12, 504000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 28, 21, 0, 26, 521000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-11-28-163246-iply',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-11-28-163246-iply',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 11, 28, 16, 32, 46, 741000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 28, 16, 32, 46, 741000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='ts-enc-dec-attention-single-gpu-2020-11-28-11-41-05-aws-training-job',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/ts-enc-dec-attention-single-gpu-2020-11-28-11-41-05-aws-training-job',display_name='single-gpu-components',trial_component_source={'SourceArn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/ts-enc-dec-attention-single-gpu-2020-11-28-11-41-05', 'SourceType': 'SageMakerTrainingJob'},status=TrialComponentStatus(primary_status='Failed',message='Status: Failed, secondary status: Failed, failure reason: AlgorithmError: ExecuteUserScriptError:\\nCommand \"/usr/bin/python3 train.py --epochs 2 --model_dir s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-11-28-11-41-05/model --non_breaking_in nonbreaking_prefix.en --non_breaking_out nonbreaking_prefix.es --nsamples 10000 --resume False --train_file cl_Inshorts.csv\".'),creation_time=datetime.datetime(2020, 11, 28, 11, 41, 37, 38000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 28, 11, 44, 21, 339000, tzinfo=tzlocal()),last_modified_by={})\n",
      "Trial Components:  TrialComponentSummary(trial_component_name='TrialComponent-2020-11-28-112734-mwkp',trial_component_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment-trial-component/trialcomponent-2020-11-28-112734-mwkp',display_name='TextPreprocessing',creation_time=datetime.datetime(2020, 11, 28, 11, 27, 34, 673000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 28, 11, 27, 34, 673000, tzinfo=tzlocal()),last_modified_by={})\n"
     ]
    }
   ],
   "source": [
    "print('Experiment: ',training_experiment.experiment_name)\n",
    "# Show the trials in the experiment\n",
    "#for trial in training_experiment.list_trials():\n",
    "    #print('Trial: ',trial.trial_name)\n",
    "\n",
    "for trial_comp in TrialComponent.list(trial_name=single_gpu_trial.trial_name):\n",
    "        print('Trial Components: ',trial_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging into W&B\n",
    "\n",
    "Goto your profile page and copy an api key. Add a new cell below and run the following:\n",
    "\n",
    "!wandb login PASTE_API_KEY_HERE\n",
    "\n",
    "You only need to run this once as it writes your credentials in your home directory.\n",
    "\n",
    "W&B looks for a file named secrets.env relative to the training script and loads them into the environment when wandb.init() is called. You can generate a secrets.env file by calling wandb.sagemaker_auth(path=\"source_dir\") in the script you use to launch your experiments. Be sure to add this file to your .gitignore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: wandb in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.10.11)\n",
      "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: watchdog>=0.8.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (0.10.2)\n",
      "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (0.19.4)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (5.0.1)\n",
      "Requirement already satisfied, skipping upgrade: Click>=7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (5.6.7)\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (3.1.11)\n",
      "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pathtools>=0.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sentry-sdk>=0.4.0->wandb) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login <API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W&B looks for a file named `secrets.env` relative to the training script and loads them into the environment when `wandb.init()` is called. You can generate a `secrets.env` file by calling `wandb.sagemaker_auth(path=\"source_dir\")` in the script you use to launch your experiments. Be sure to add this file to your .gitignore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.sagemaker_auth(path=\"Encoder_Decoder_Attention/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training\n",
    "\n",
    "Script mode is a training script format for TensorFlow that lets you execute any TensorFlow training script in SageMaker with minimal modification. The SageMaker Python SDK handles transferring your script to a SageMaker training instance. On the training instance, SageMaker's native TensorFlow support sets up training-related environment variables and executes your training script. In this tutorial, we use the SageMaker Python SDK to launch a training job.\n",
    "\n",
    "Script mode supports training with a Python script, a Python module, or a shell script.\n",
    "\n",
    "This project's training script was adapted from the Tensorflow model of a Transformer, we develop in a previous post (mentioned previously). We have modified it to handle: \n",
    "- the ``train_file``, ``non_breaking_in``and ``non_breaking_out`` parameters passed in with the values of the training data-set, the non breaking prefixes for the input data and the non breaking prefixes for the output data.\n",
    "\n",
    "- the ``data_dir`` parameter passed in by SageMaker with the value of the enviroment variable `SM_CHANNEL_TRAINING`. This is an S3 path used for input data sharing during training.\n",
    "\n",
    "- the ``model_dir`` parameter passed in by SageMaker. This is an S3 path which can be used for data sharing during distributed training and checkpointing and/or model persistence. We have also added an argument-parsing function to handle processing training-related variables.\n",
    "\n",
    "- the local checkpoint path to store the model checkpoints during training. We use the default value ``/opt/ml/checkpoints`` that will be uploaded to S3. We comment this behavior later when defining our estimator.\n",
    "\n",
    "- At the end of the training job we have added a step to export the trained model, only the weights, to the path stored in the environment variable ``SM_MODEL_DIR``, which always points to ``/opt/ml/model``. This is critical because SageMaker uploads all the model artifacts in this folder to S3 at end of training.\n",
    "\n",
    "- the ``output_data_dir`` parameter passed in by SageMaker with the value of the enviroment variable `SM_OUTPUT_DATA_DIR`. This is a folder path used to save output data from our model. This folder will be uploaded to S3 to store the output.tar.zip. In our case we need to save the tokenizer for the input texts, the tokenizer for the outputs, the input and output vocab size and the tokens for ``eos`` and ``sos``. \n",
    "\n",
    "\n",
    "In addition to the train.py file, our source code folder includes the files:\n",
    "- model.py: Tensorflow model definition\n",
    "- utils.py: utility functions to process the text data \n",
    "- utils_train.py: contains functions to calculate the loss and learning rate scheduler.\n",
    "\n",
    "\n",
    "Here is the entire script for the train.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgc\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m layers\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m backend \u001b[34mas\u001b[39;49;00m K\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\r\n",
      "\r\n",
      "\u001b[37m# To install tensorflow_datasets\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minstall\u001b[39;49;00m(package):\r\n",
      "    subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-q\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, package])\r\n",
      "\r\n",
      "\u001b[37m# Install the library tensorflow_datasets\u001b[39;49;00m\r\n",
      "install(\u001b[33m'\u001b[39;49;00m\u001b[33mdatasets\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "install(\u001b[33m'\u001b[39;49;00m\u001b[33mrouge_score\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "install(\u001b[33m'\u001b[39;49;00m\u001b[33mwandb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m word_tokenize, tensor_to_text\r\n",
      "\u001b[37m#from utils_train import loss_function, CustomSchedule\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menc_dec_att_model\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Encoder, Decoder, BahdanauAttention\r\n",
      "\r\n",
      "\u001b[37m#NUM_SAMPLES = 80000 #40000\u001b[39;49;00m\r\n",
      "\u001b[37m#MAX_VOCAB_SIZE = 2**14\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#BATCH_SIZE = 64  # Batch size for training.\u001b[39;49;00m\r\n",
      "\u001b[37m#EPOCHS = 10  # Number of epochs to train for.\u001b[39;49;00m\r\n",
      "\u001b[37m#MAX_LENGTH = 15\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_train_data\u001b[39;49;00m(training_dir, train_file, nsamples):\r\n",
      "\r\n",
      "    train_filenamepath = os.path.abspath(os.path.join(training_dir, train_file))\r\n",
      "    train_df=pd.read_csv(train_filenamepath, header=\u001b[34m0\u001b[39;49;00m, usecols=[\u001b[34m0\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m], \r\n",
      "               nrows=nsamples)\r\n",
      "\r\n",
      "    \u001b[37m# Load the validation dataset: text and summary\u001b[39;49;00m\r\n",
      "    \u001b[37m#valid_df=pd.read_csv(valid_filenamepath, header=0, usecols=[0,1], \u001b[39;49;00m\r\n",
      "    \u001b[37m#               nrows=NUM_SAMPLES)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of train sentences: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[36mlen\u001b[39;49;00m(train_df))\r\n",
      "    \u001b[37m#print('Number of validation sentences: ',len(valid_df))\u001b[39;49;00m\r\n",
      "\r\n",
      "    input_train = \u001b[33m'\u001b[39;49;00m\u001b[33m<start> \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+train_df[\u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].values+\u001b[33m'\u001b[39;49;00m\u001b[33m <end>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    target_train = \u001b[33m'\u001b[39;49;00m\u001b[33m<start> \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+train_df[\u001b[33m'\u001b[39;49;00m\u001b[33msummary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].values+\u001b[33m'\u001b[39;49;00m\u001b[33m <end>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m input_train, target_train\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_datasets\u001b[39;49;00m(encoder_inputs, decoder_outputs, train_frac, batch_size, seed):\r\n",
      "    \u001b[37m# Split the dataset into a train and validation dataset\u001b[39;49;00m\r\n",
      "    train_enc_inputs, val_enc_inputs, train_dec_outputs, val_dec_outputs, = train_test_split(encoder_inputs, decoder_outputs,\r\n",
      "                                                                                       train_size=train_frac,random_state=seed, shuffle=\u001b[34mTrue\u001b[39;49;00m )\r\n",
      "    BUFFER_SIZE = \u001b[36mlen\u001b[39;49;00m(train_enc_inputs)\r\n",
      "    steps_per_epoch = \u001b[36mlen\u001b[39;49;00m(train_enc_inputs)//batch_size\r\n",
      "    val_steps_per_epoch = \u001b[36mlen\u001b[39;49;00m(val_enc_inputs)//batch_size\r\n",
      "\r\n",
      "    \u001b[37m# Define a train dataset \u001b[39;49;00m\r\n",
      "    dataset = tf.data.Dataset.from_tensor_slices(\r\n",
      "                                    (train_enc_inputs, train_dec_outputs))\r\n",
      "    dataset = dataset.shuffle(train_enc_inputs.shape[\u001b[34m0\u001b[39;49;00m], reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m).batch(\r\n",
      "                                    batch_size, drop_remainder=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    \u001b[37m# Define a train dataset \u001b[39;49;00m\r\n",
      "    val_dataset = tf.data.Dataset.from_tensor_slices(\r\n",
      "                                    (val_enc_inputs, val_dec_outputs))\r\n",
      "    val_dataset = val_dataset.shuffle(val_enc_inputs.shape[\u001b[34m0\u001b[39;49;00m], reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m).batch(\r\n",
      "                                    batch_size, drop_remainder=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset, val_dataset, steps_per_epoch, val_steps_per_epoch\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_step\u001b[39;49;00m(inp, targ, enc_hidden):\r\n",
      "  loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.GradientTape() \u001b[34mas\u001b[39;49;00m tape:\r\n",
      "    enc_output, enc_hidden = encoder(inp, enc_hidden)\r\n",
      "\r\n",
      "    dec_hidden = enc_hidden\r\n",
      "\r\n",
      "    dec_input = tf.expand_dims([tokenizer_outputs.word_index[\u001b[33m'\u001b[39;49;00m\u001b[33m<start>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]] * BATCH_SIZE, \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Teacher forcing - feeding the target as the next input\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m t \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, targ.shape[\u001b[34m1\u001b[39;49;00m]):\r\n",
      "      \u001b[37m# passing enc_output to the decoder\u001b[39;49;00m\r\n",
      "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\r\n",
      "\r\n",
      "      loss += loss_function(targ[:, t], predictions)\r\n",
      "\r\n",
      "      \u001b[37m# using teacher forcing\u001b[39;49;00m\r\n",
      "      dec_input = tf.expand_dims(targ[:, t], \u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "  batch_loss = (loss / \u001b[36mint\u001b[39;49;00m(targ.shape[\u001b[34m1\u001b[39;49;00m]))\r\n",
      "\r\n",
      "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
      "  gradients = tape.gradient(loss, variables)\r\n",
      "  optimizer.apply_gradients(\u001b[36mzip\u001b[39;49;00m(gradients, variables))\r\n",
      "\r\n",
      "  train_loss.update_state(batch_loss)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m batch_loss\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32meval_step\u001b[39;49;00m(inp, targ, enc_hidden):\r\n",
      "    loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    enc_output, enc_hidden = encoder(inp, enc_hidden)\r\n",
      "    dec_hidden = enc_hidden\r\n",
      "    dec_input = tf.expand_dims([tokenizer_outputs.word_index[\u001b[33m'\u001b[39;49;00m\u001b[33m<start>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]] * BATCH_SIZE, \u001b[34m1\u001b[39;49;00m)\r\n",
      "    result_ids = tf.expand_dims([tokenizer_outputs.word_index[\u001b[33m'\u001b[39;49;00m\u001b[33m<start>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]] * BATCH_SIZE, \u001b[34m0\u001b[39;49;00m).numpy()\r\n",
      "    \u001b[37m#print(result_ids.shape)\u001b[39;49;00m\r\n",
      "    \u001b[37m# Teacher forcing - feeding the target as the next input\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m t \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, targ.shape[\u001b[34m1\u001b[39;49;00m]):\r\n",
      "      \u001b[37m# passing enc_output to the decoder\u001b[39;49;00m\r\n",
      "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\r\n",
      "      loss += loss_function(targ[:, t], predictions)\r\n",
      "      \u001b[37m# using teacher forcing\u001b[39;49;00m\r\n",
      "      dec_input = tf.expand_dims(targ[:, t], \u001b[34m1\u001b[39;49;00m)\r\n",
      "      \u001b[37m#\u001b[39;49;00m\r\n",
      "      \u001b[37m#print(predictions.shape)\u001b[39;49;00m\r\n",
      "      predicted_ids = tf.argmax(predictions, axis=\u001b[34m1\u001b[39;49;00m).numpy()\r\n",
      "      \u001b[34mif\u001b[39;49;00m result_ids \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "          result_ids = predicted_ids\r\n",
      "      \u001b[34melse\u001b[39;49;00m:\r\n",
      "          result_ids = np.vstack((result_ids , predicted_ids ))\r\n",
      "          \u001b[37m#np.concatenate(([result_ids , predicted_ids ]), axis=1)\u001b[39;49;00m\r\n",
      "      \u001b[37m#result_ids = result_ids.T\u001b[39;49;00m\r\n",
      "      \u001b[37m#print(result_ids.shape, type(result_ids))\u001b[39;49;00m\r\n",
      "\r\n",
      "    result_ids = result_ids.T\r\n",
      "    \u001b[37m#print('Fin: ',result_ids.shape, type(result_ids))\u001b[39;49;00m\r\n",
      "\r\n",
      "    batch_loss = (loss / \u001b[36mint\u001b[39;49;00m(targ.shape[\u001b[34m1\u001b[39;49;00m]))\r\n",
      "    val_loss.update_state(batch_loss)\r\n",
      "   \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m batch_loss, result_ids\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain_train\u001b[39;49;00m(dataset, val_dataset, n_epochs, steps_per_epoch, val_steps_per_epoch,\r\n",
      "               save_checkpoints=\u001b[34mFalse\u001b[39;49;00m, logging= \u001b[34mFalse\u001b[39;49;00m, print_every=\u001b[34m50\u001b[39;49;00m):\r\n",
      "  \u001b[33m''' Train the transformer model for n_epochs using the data generator dataset'''\u001b[39;49;00m\r\n",
      "  train_losses = []\r\n",
      "  val_losses = []\r\n",
      "  val_metric = []\r\n",
      "\r\n",
      "  \u001b[37m# In every epoch\u001b[39;49;00m\r\n",
      "  \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(n_epochs):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch+\u001b[34m1\u001b[39;49;00m))\r\n",
      "    start = time.time()\r\n",
      "    \u001b[37m# Reset the losss and accuracy calculations\u001b[39;49;00m\r\n",
      "    train_loss.reset_states()\r\n",
      "    \u001b[37m#Initialize the encoder states\u001b[39;49;00m\r\n",
      "    enc_hidden = encoder.initialize_hidden_state()\r\n",
      "    total_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m (batch, (inp, targ)) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(dataset.take(steps_per_epoch)):\r\n",
      "        batch_loss = train_step(inp, targ, enc_hidden)\r\n",
      "        total_loss += batch_loss\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m batch % print_every == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            train_losses.append(train_loss.result())\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain: Epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Batch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Loss \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch + \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                   batch,\r\n",
      "                                                   batch_loss.numpy()))\r\n",
      "            \u001b[37m# Register in wandb\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m logging:\r\n",
      "                wandb.log({\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining Loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: train_loss.result()})\r\n",
      "\r\n",
      "    \u001b[37m# Reset the validation losss and accuracy calculations\u001b[39;49;00m\r\n",
      "    val_loss.reset_states()\r\n",
      "    total_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[37m# Evaluation loop\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m (batch, (inp, targ)) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(val_dataset.take(val_steps_per_epoch)):\r\n",
      "        batch_loss, preds = eval_step(inp, targ, enc_hidden)\r\n",
      "        total_loss += batch_loss\r\n",
      "        \u001b[37m# Add the predictions to the metric\u001b[39;49;00m\r\n",
      "        \u001b[37m#Convert sequence to text\u001b[39;49;00m\r\n",
      "        \u001b[37m#print(preds.shape, targ.shape)\u001b[39;49;00m\r\n",
      "        predictions = tensor_to_text(tokenizer_outputs, preds)\r\n",
      "        \u001b[37m#references = tokenizer_outputs.sequences_to_texts(targ)\u001b[39;49;00m\r\n",
      "        references = tensor_to_text(tokenizer_outputs, targ.numpy())\r\n",
      "        metric.add_batch(predictions=predictions, references=references)\r\n",
      "\r\n",
      "    val_losses.append(val_loss.result())\r\n",
      "    \u001b[37m# compute the metric\u001b[39;49;00m\r\n",
      "    metric_results = metric.compute()\r\n",
      "    metric_score = parse_score(metric_results)\r\n",
      "    \u001b[37m# Save the validation metric\u001b[39;49;00m\r\n",
      "    val_metric.append(metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrouge1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[37m# Register in wandb\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m logging:\r\n",
      "        wandb.log({\u001b[33m\"\u001b[39;49;00m\u001b[33mValidation Loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: val_loss.result(), \u001b[37m#})\u001b[39;49;00m\r\n",
      "                   \u001b[33m\"\u001b[39;49;00m\u001b[33mRouge1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrouge1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "                   \u001b[33m\"\u001b[39;49;00m\u001b[33mRouge2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrouge2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "                   \u001b[33m\"\u001b[39;49;00m\u001b[33mRougeL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrougeL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]})\r\n",
      "\r\n",
      "    \u001b[37m#Show Validation results\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mValidation: Epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Batch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Loss \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m Rouge1 \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m Rouge2 \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m RougeL \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                epoch+\u001b[34m1\u001b[39;49;00m, batch, val_loss.result(),metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrouge1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrouge2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "                metric_score[\u001b[33m'\u001b[39;49;00m\u001b[33mrougeL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "\r\n",
      "    \u001b[37m# Checkpoint the model on every epoch    \u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m ((epoch + \u001b[34m1\u001b[39;49;00m) % \u001b[34m2\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m save_checkpoints:    \r\n",
      "      ckpt_save_path = ckpt_manager.save()\r\n",
      "      \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving checkpoint for epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch+\u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                        ckpt_save_path))\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTime for 1 epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m secs\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(time.time() - start))\r\n",
      "\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m train_losses, val_losses, val_metric\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mloss_function\u001b[39;49;00m(target, pred):\r\n",
      "    mask = tf.math.logical_not(tf.math.equal(target, \u001b[34m0\u001b[39;49;00m))\r\n",
      "    loss_ = loss_object(target, pred)\r\n",
      "    \r\n",
      "    mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
      "    loss_ *= mask\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.reduce_mean(loss_)\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mCustomSchedule\u001b[39;49;00m(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, d_model, warmup_steps=\u001b[34m4000\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(CustomSchedule, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.d_model = tf.cast(d_model, tf.float32)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.warmup_steps = warmup_steps\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__call__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, step):\r\n",
      "        arg1 = tf.math.rsqrt(step)\r\n",
      "        arg2 = step * (\u001b[36mself\u001b[39;49;00m.warmup_steps**-\u001b[34m1.5\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.math.rsqrt(\u001b[36mself\u001b[39;49;00m.d_model) * tf.math.minimum(arg1, arg2)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    \u001b[37m# Install tensorflow_datasets\u001b[39;49;00m\r\n",
      "    \u001b[37m#install('tensorflow_datasets')\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments when the script\u001b[39;49;00m\r\n",
      "    \u001b[37m# is executed. Here we set up an argument parser to easily access the parameters.\u001b[39;49;00m\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--text-max-len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m60\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput max sequence length for training (default: 60)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--summ-max-len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m15\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mtarget max sequence length for training (default: 60)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--nsamples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of samples to train (default: 20000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--resume\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mResume training from the latest checkpoint (default: False)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Data parameters                    \u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_file\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTraining data file name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--OOV-token\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m<unk>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mOut of vocabulary token \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_frac\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.85\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mFraction of training data (default: 0.85)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--embedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mEmbedding dimension (default: 128)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gru_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m512\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mUnits of the Gru layer (default: 512)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--att_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mAttention dimension (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--vocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the vocabulary (default: 10000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\u001b[37m#    parser.add_argument('--n_layers', type=int, default=4, metavar='N',\u001b[39;49;00m\r\n",
      "\u001b[37m#                        help='number of layers (default: 4)')\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# SageMaker Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.sm_model_dir, args.model_dir)\r\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet the train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    input_data, target_data = get_train_data(args.data_dir, args.train_file, args.nsamples)\r\n",
      "\r\n",
      "    \u001b[37m# Tokenize and pad the input sequences\u001b[39;49;00m\r\n",
      "    encoder_inputs, tokenizer_inputs, input_max_length = word_tokenize(input_data,args.vocab_size,args.text_max_len, args.OOV_token)\r\n",
      "    \u001b[37m# Tokenize and pad the outputs sequences\u001b[39;49;00m\r\n",
      "    decoder_outputs, tokenizer_outputs, output_max_length = word_tokenize(target_data,args.vocab_size, args.summ_max_len, args.OOV_token)\r\n",
      "    input_vocab_size = tokenizer_inputs.num_words\r\n",
      "    output_vocab_size = tokenizer_outputs.num_words\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInput vocab: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,input_vocab_size)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOutput vocab: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,output_vocab_size)\r\n",
      "    \r\n",
      "    dataset, val_dataset, steps_per_epoch, val_steps_per_epoch = get_datasets(encoder_inputs, decoder_outputs, args.train_frac, \r\n",
      "                                                                    args.batch_size, args.seed)\r\n",
      "\r\n",
      "    \u001b[37m# Create the metric\u001b[39;49;00m\r\n",
      "    metric = datasets.load_metric(\u001b[33m'\u001b[39;49;00m\u001b[33mrouge\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Clean the session\u001b[39;49;00m\r\n",
      "    tf.keras.backend.clear_session()\r\n",
      "    \u001b[37m# Create the Transformer model\u001b[39;49;00m\r\n",
      "    encoder = Encoder(input_vocab_size, args.embedding_dim, args.gru_units, args.batch_size)\r\n",
      "    attention_layer = BahdanauAttention(args.att_units)\r\n",
      "    decoder = Decoder(output_vocab_size, args.embedding_dim, args.gru_units, args.batch_size)\r\n",
      "\r\n",
      "    \u001b[37m# Define a categorical cross entropy loss\u001b[39;49;00m\r\n",
      "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                                reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Define a metric to store the mean loss of every epoch\u001b[39;49;00m\r\n",
      "    train_loss = tf.keras.metrics.Mean(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Define a matric to save the accuracy in every epoch\u001b[39;49;00m\r\n",
      "    \u001b[37m#train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\u001b[39;49;00m\r\n",
      "    \u001b[37m# Define a metric to store the mean loss of every epoch\u001b[39;49;00m\r\n",
      "    val_loss = tf.keras.metrics.Mean(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mval_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Define a matric to save the accuracy in every epoch\u001b[39;49;00m\r\n",
      "    \u001b[37m#val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Create the scheduler for learning rate decay\u001b[39;49;00m\r\n",
      "    \u001b[37m#leaning_rate = CustomSchedule(D_MODEL)\u001b[39;49;00m\r\n",
      "    \u001b[37m# Create the Adam optimizer\u001b[39;49;00m\r\n",
      "    optimizer = tf.keras.optimizers.Adam(args.learning_rate,\r\n",
      "                                        beta_1=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                                        beta_2=\u001b[34m0.98\u001b[39;49;00m,\r\n",
      "                                        epsilon=\u001b[34m1e-9\u001b[39;49;00m)\r\n",
      "       \r\n",
      "    \u001b[37m#Create the Checkpoint \u001b[39;49;00m\r\n",
      "    ckpt = tf.train.Checkpoint(optimizer=optimizer,\r\n",
      "                                    encoder=encoder,\r\n",
      "                                    decoder=decoder)\r\n",
      "\r\n",
      "    ckpt_manager = tf.train.CheckpointManager(ckpt, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, max_to_keep=\u001b[34m2\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m ckpt_manager.latest_checkpoint \u001b[35mand\u001b[39;49;00m args.resume:\r\n",
      "        ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLast checkpoint restored.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining the model ....\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\r\n",
      "    train_losses, val_losses, val_metric = main_train(dataset, val_dataset, args.epochs, steps_per_epoch, \r\n",
      "                                                  val_steps_per_epoch, save_checkpoints=\u001b[34mFalse\u001b[39;49;00m, logging= \u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[37m# Save the while model\u001b[39;49;00m\r\n",
      "    \u001b[37m# Save the entire model to a HDF5 file\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving the model ....\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    encoder.save_weights(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mencoder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), overwrite=\u001b[34mTrue\u001b[39;49;00m, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    attention_layer.save_weights(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mattention\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), overwrite=\u001b[34mTrue\u001b[39;49;00m, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    decoder.save_weights(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mdecoder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), overwrite=\u001b[34mTrue\u001b[39;49;00m, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Save the parameters used to construct the model\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model parameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model_info_path = os.path.join(args.output_data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size_enc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_vocab_size,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size_dec\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: output_vocab_size,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.embedding_dim,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mgru_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.gru_units,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33matt_units\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.att_units,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.batch_size\r\n",
      "        }\r\n",
      "        pickle.dump(model_info, f)\r\n",
      "          \r\n",
      "\t\u001b[37m# Save the tokenizers with the vocabularies\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving the dictionaries ....\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    vocabulary_in = os.path.join(args.output_data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33min_vocab.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(vocabulary_in, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(tokenizer_inputs, f)\r\n",
      "\r\n",
      "    vocabulary_out = os.path.join(args.output_data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mout_vocab.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(vocabulary_out, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(tokenizer_outputs, f)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'Encoder_Decoder_Attention/train/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our source code needs the tensorflow_dataset library and it is not include in the Tensorflow 2.1. container image provided by SageMaker. To solve this issue we explicitly install it in our train.py file using the command `subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container where the model will run, uploading your script or source code to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* `source_dir`and `entry_point`, the folder with the source code and the file to run the training.\n",
    "* `framework_version` is the tensorflow version we want to run our code.\n",
    "* `py_version` is set to `'py3'` to indicate that we are using script mode since legacy mode supports only Python 2. Though Python 2 will be deprecated soon, you can use script mode with Python 2 by setting `py_version` to `'py2'` and `script_mode` to `True`.\n",
    "* `code_location` is a S3 folder URI where the `source_dir` will be upload. When the instace starts the content of that folder will be downloaded to a local path, `opt/ml/code`. The `entry_point`, our main code or function, has to be included in that folder.\n",
    "* `output_path` is the S3 path where all the outputs of our training job will be uploaded when the training ends. In our example we will upload to this S3 folder the local content in the folders `SM_MODEL_DIR` and `SM_OUTPUT_DATA_DIR`.\n",
    "* the `checkpoint_local_path`and `checkpoint_s3_uri` parameters will be explained in the next section **\"Resume training from a checkpoint\"**\n",
    "* `script_mode = True` to set script mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the type of instance to use\n",
    "#instance_type='ml.m4.4xlarge'\n",
    "instance_type='ml.p2.xlarge'\n",
    "#instance_type='local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important parameter of our Tensorflow estimator is the `instance_type` that is the type of \"virtual machine\" where the container will run. The values we play around in this project are:\n",
    "- local: The container will run locally on the notebook instance. this is very useful to debug or verify that our estimator definition is correct and the train.py runs successfully. It is much more faster to run the container locally, the start up time for a remote instance is too long when you are coding and debugging.\n",
    "- ml.mX.Yxlarge: It is a CPU instance, when you are running your code for a short train, maybe for validation purposes. Check AWS documentation for a list of alternative instance.\n",
    "- ml.p2.xlarge: This instance use a GPU and it is the preferred one when you want to launch a long running training.\n",
    "\n",
    "When running in local mode, some estimator functionalities are not available like uploading the checkpoints to S3 and its parameters should not be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we want to mention the definition of metrics. Using a dictionary, we can define a metric name and the regular expression to extract its value from the messages the training script writes on the logs or the stdout during training. Later we can see those metrics in the SageMaker console. We show you how to do it in a following section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to search for\n",
    "metric_definitions = [{'Name': 'train_loss', 'Regex': 'Train: [0-9a-zA-Z. ]+ Loss ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_loss', 'Regex': 'Validation: [0-9a-zA-Z. ]+ Loss ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_rouge1', 'Regex': 'Validation: [0-9a-zA-Z. ]+ Rouge1 ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_rouge2', 'Regex': 'Validation: [0-9a-zA-Z. ]+ Rouge2 ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_rougel', 'Regex': 'Validation: [0-9a-zA-Z. ]+ RougeL ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Tensorflow estimator using a Tensorflow 2.1 container\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir=\"Encoder_Decoder_Attention/train\",\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       framework_version='2.2.0',\n",
    "                       py_version='py37',\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='ts-enc-dec-attention',\n",
    "                       script_mode= True,\n",
    "                       #checkpoint_local_path = 'ckpt', #Use default value /opt/ml/checkpoint\n",
    "                       #checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       metric_definitions = metric_definitions, \n",
    "                       hyperparameters={\n",
    "                        'epochs': 3,\n",
    "                        'nsamples': 25000,\n",
    "                        'resume': False,\n",
    "                        'gru_units': 1024,\n",
    "                        'att_units': 128,\n",
    "                        'vocab_size': 16384,\n",
    "                        'train_file': 'cl_Inshorts.csv'\n",
    "                       })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.2.0-gpu-py37'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.training_image_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training job: ``fit``\n",
    "\n",
    "To start a training job, we call `estimator.fit` method with the a few parameter values.\n",
    "\n",
    "- An S3 location is used here as the input. `fit` creates a default channel named `'training'`, which points to this S3 location. In the training script we can access the training data from the local location stored in `SM_CHANNEL_TRAINING`. `fit` accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "- `job_name` the name for the training job.\n",
    "- `experiment_config` the dictionary with the name of the experiment and trial to attach this job to.\n",
    "\n",
    "\n",
    "When training starts, the TensorFlow container executes `train.py`, passing `hyperparameters` and `model_dir` from the estimator as script arguments. Because we didn't explicitly define it, `model_dir` defaults to `s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>/model`, so the script execution is as follows:\n",
    "```bash\n",
    "python train.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>/model --epochs=1 --nsamples=5000 ...\n",
    "```\n",
    "\n",
    "When training is complete, the training job will upload the saved model and other output artifacts to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55\n"
     ]
    }
   ],
   "source": [
    "# Set the job name and show it\n",
    "job_name = '{}-{}'.format(trial_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit to train a model with TensorFlow 2.1 scroipt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-01 17:39:58 Starting - Starting the training job...\n",
      "2020-12-01 17:40:01 Starting - Launching requested ML instances......\n",
      "2020-12-01 17:41:19 Starting - Preparing the instances for training.........\n",
      "2020-12-01 17:42:38 Downloading - Downloading input data...\n",
      "2020-12-01 17:43:00 Training - Downloading the training image............\n",
      "2020-12-01 17:45:10 Training - Training image download completed. Training in progress.\u001b[34m2020-12-01 17:45:16,820 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-12-01 17:45:17,537 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting rouge_score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets\n",
      "  Downloading datasets-1.1.3-py3-none-any.whl (153 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34m  Downloading wandb-0.10.11-py2.py3-none-any.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2020.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (0.3.3)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (1.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (1.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/site-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.2)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 5)) (2.23.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\u001b[0m\n",
      "\u001b[34mCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.1-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting Click>=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\u001b[0m\n",
      "\u001b[34mCollecting watchdog>=0.8.3\n",
      "  Downloading watchdog-0.10.4.tar.gz (98 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=0.4.0\n",
      "  Downloading sentry_sdk-0.19.4-py2.py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 6)) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 6)) (5.7.2)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow->-r requirements.txt (line 3)) (50.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from nltk->rouge_score->-r requirements.txt (line 4)) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-manylinux2014_x86_64.whl (719 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 5)) (1.25.10)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools>=0.1.1\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (4.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, watchdog, subprocess32, promise, pathtools\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434675 sha256=fe62e0d543eb8db1ef39238ae7cedff871b2c7cd55799c0a1308da216c304682\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "  Building wheel for watchdog (setup.py): started\n",
      "  Building wheel for watchdog (setup.py): finished with status 'done'\n",
      "  Created wheel for watchdog: filename=watchdog-0.10.4-py3-none-any.whl size=74841 sha256=64b510d56ed48de4cb84fd6430d588daa3b6591fa59cf3b1cb3dc7eb7381dcd3\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/e0/c5/33f9291a7c355bccc991701ffd4bf0f711fe4674aaef2b2e5c\n",
      "  Building wheel for subprocess32 (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6487 sha256=2d9e6d26e83eff6c91fcccc0251e582532aede20fe9aba799df027a877945a86\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=e7ca9a44a3f6589b9bf4834cd64ff63f14bb62c8117be635912b904d1083526e\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "  Building wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8785 sha256=3be10588212fe01b71c7e7c3348b8d6235bbe9c342d580673254f816adcecf16\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk watchdog subprocess32 promise pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorflow-estimator, tensorboard, tensorflow, Click, regex, tqdm, nltk, rouge-score, dill, multiprocess, pyarrow, xxhash, datasets, configparser, smmap, gitdb, GitPython, pathtools, watchdog, docker-pycreds, subprocess32, sentry-sdk, shortuuid, promise, wandb\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.2.0\n",
      "    Uninstalling tensorflow-estimator-2.2.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.2.2\n",
      "    Uninstalling tensorboard-2.2.2:\n",
      "      Successfully uninstalled tensorboard-2.2.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed Click-7.1.2 GitPython-3.1.11 configparser-5.0.1 datasets-1.1.3 dill-0.3.3 docker-pycreds-0.4.0 gitdb-4.0.5 multiprocess-0.70.11.1 nltk-3.5 pathtools-0.1.2 promise-2.3 pyarrow-2.0.0 regex-2020.11.13 rouge-score-0.0.4 sentry-sdk-0.19.4 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 tensorboard-2.4.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 tqdm-4.49.0 wandb-0.10.11 watchdog-0.10.4 xxhash-2.0.0\u001b[0m\n",
      "\u001b[34m2020-12-01 17:46:04,822 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": false,\n",
      "        \"gru_units\": 1024,\n",
      "        \"nsamples\": 25000,\n",
      "        \"train_file\": \"cl_Inshorts.csv\",\n",
      "        \"vocab_size\": 16384,\n",
      "        \"att_units\": 128,\n",
      "        \"model_dir\": \"s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model\",\n",
      "        \"epochs\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"att_units\":128,\"epochs\":3,\"gru_units\":1024,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model\",\"nsamples\":25000,\"resume\":false,\"train_file\":\"cl_Inshorts.csv\",\"vocab_size\":16384}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"att_units\":128,\"epochs\":3,\"gru_units\":1024,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model\",\"nsamples\":25000,\"resume\":false,\"train_file\":\"cl_Inshorts.csv\",\"vocab_size\":16384},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--att_units\",\"128\",\"--epochs\",\"3\",\"--gru_units\",\"1024\",\"--model_dir\",\"s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model\",\"--nsamples\",\"25000\",\"--resume\",\"False\",\"--train_file\",\"cl_Inshorts.csv\",\"--vocab_size\",\"16384\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=false\u001b[0m\n",
      "\u001b[34mSM_HP_GRU_UNITS=1024\u001b[0m\n",
      "\u001b[34mSM_HP_NSAMPLES=25000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=cl_Inshorts.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VOCAB_SIZE=16384\u001b[0m\n",
      "\u001b[34mSM_HP_ATT_UNITS=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 train.py --att_units 128 --epochs 3 --gru_units 1024 --model_dir s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model --nsamples 25000 --resume False --train_file cl_Inshorts.csv --vocab_size 16384\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /usr/local/lib/python3.7/site-packages (1.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /usr/local/lib/python3.7/site-packages (from datasets) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/site-packages (from datasets) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/site-packages (from datasets) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /usr/local/lib/python3.7/site-packages (from datasets) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/site-packages (from datasets) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/site-packages (from datasets) (2.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from datasets) (1.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/site-packages (from datasets) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.25.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->datasets) (2020.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rouge_score in /usr/local/lib/python3.7/site-packages (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from rouge_score) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (from rouge_score) (3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/site-packages (from rouge_score) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.7/site-packages (from rouge_score) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex in /usr/local/lib/python3.7/site-packages (from nltk->rouge_score) (2020.11.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from nltk->rouge_score) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from nltk->rouge_score) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from nltk->rouge_score) (0.16.0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRequirement already satisfied: wandb in /usr/local/lib/python3.7/site-packages (0.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/site-packages (from wandb) (5.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/site-packages (from wandb) (0.19.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/site-packages (from wandb) (3.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/site-packages (from wandb) (3.1.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/site-packages (from wandb) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/site-packages (from wandb) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from wandb) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.7/site-packages (from wandb) (0.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/site-packages (from wandb) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from wandb) (2.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/site-packages (from wandb) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/site-packages (from wandb) (5.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/site-packages (from wandb) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/site-packages (from wandb) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/site-packages (from wandb) (2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/site-packages (from sentry-sdk>=0.4.0->wandb) (1.25.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /usr/local/lib/python3.7/site-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.7/site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.12.0->wandb) (50.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\u001b[0m\n",
      "\u001b[34m/opt/ml/model s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-17-39-55/model\u001b[0m\n",
      "\u001b[34mGet the train data\u001b[0m\n",
      "\u001b[34mNumber of train sentences:  25000\u001b[0m\n",
      "\u001b[34mInput vocab:  16384\u001b[0m\n",
      "\u001b[34mOutput vocab:  16384\u001b[0m\n",
      "\u001b[34mTraining the model ....\u001b[0m\n",
      "\u001b[34mStarting epoch 1\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 0 Loss 6.5199\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 50 Loss 4.8500\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 100 Loss 4.6541\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 150 Loss 4.8634\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 200 Loss 4.6487\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 250 Loss 4.7434\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 300 Loss 4.2898\u001b[0m\n",
      "\u001b[34mValidation: Epoch 1 Batch 57 Loss 4.5026 Rouge1 21.2393 Rouge2 1.0395 RougeL 21.1575\n",
      "\u001b[0m\n",
      "\u001b[34mTime for 1 epoch: 203.8983669281006 secs\n",
      "\u001b[0m\n",
      "\u001b[34mStarting epoch 2\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 0 Loss 4.3121\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 50 Loss 4.2898\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 100 Loss 4.4082\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 150 Loss 4.3236\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 200 Loss 4.3470\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 250 Loss 4.3874\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 300 Loss 4.1633\u001b[0m\n",
      "\u001b[34mValidation: Epoch 2 Batch 57 Loss 4.4181 Rouge1 22.2113 Rouge2 1.2583 RougeL 21.9639\n",
      "\u001b[0m\n",
      "\u001b[34mTime for 1 epoch: 201.2836935520172 secs\n",
      "\u001b[0m\n",
      "\u001b[34mStarting epoch 3\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 0 Loss 4.2903\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 50 Loss 4.2684\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 100 Loss 4.3145\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 150 Loss 4.2512\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 200 Loss 4.2028\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 250 Loss 4.0627\u001b[0m\n",
      "\u001b[34mTrain: Epoch 3 Batch 300 Loss 4.2234\u001b[0m\n",
      "\u001b[34mValidation: Epoch 3 Batch 57 Loss 4.3559 Rouge1 23.0092 Rouge2 1.4952 RougeL 22.7670\n",
      "\u001b[0m\n",
      "\u001b[34mTime for 1 epoch: 201.30677676200867 secs\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model ....\u001b[0m\n",
      "\u001b[34mSaving the model parameters\u001b[0m\n",
      "\u001b[34mSaving the dictionaries ....\u001b[0m\n",
      "\u001b[34m2020-12-01 17:56:30,213 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2020-12-01 17:56:30,214 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-12-01 17:56:34 Uploading - Uploading generated training model\n",
      "2020-12-01 17:57:02 Completed - Training job completed\n",
      "Training seconds: 864\n",
      "Billable seconds: 864\n"
     ]
    }
   ],
   "source": [
    "# Call the fit method to launch the training job\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name, \n",
    "              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiment, then you can view it and its trials from SageMaker Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f3c547d59e8>,experiment_name='ts-enc-dec-attention',experiment_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment/ts-enc-dec-attention',display_name='ts-enc-dec-attention',description='Experiment to track trainings on my tensorflow Transformer Eng-Spa',creation_time=datetime.datetime(2020, 11, 28, 11, 26, 31, 51000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 12, 1, 17, 32, 44, 322000, tzinfo=tzlocal()),last_modified_by={},response_metadata={'RequestId': 'f70fe463-bcbc-4818-a108-3633ade63a51', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f70fe463-bcbc-4818-a108-3633ade63a51', 'content-type': 'application/x-amz-json-1.1', 'content-length': '92', 'date': 'Tue, 01 Dec 2020 18:02:15 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trial\n",
    "single_gpu_trial.save()\n",
    "# Save the experiment\n",
    "training_experiment.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show metrics from SageMaker Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the metrics that a training job emits in real time in the **CloudWatch console**:\n",
    "- Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/.\n",
    "- Choose Metrics, then choose /aws/sagemaker/TrainingJobs.\n",
    "- Choose TrainingJobName.\n",
    "- On the All metrics tab, choose the names of the training metrics that you want to monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another option is to monitor the metrics by using the **SageMaker console**.\n",
    "- Open the SageMaker console at https://console.aws.amazon.com/sagemaker/.\n",
    "- Choose Training jobs, then choose the training job whose metrics you want to see.\n",
    "- Choose TrainingJobName.\n",
    "- In the Monitor section, you can review the graphs of instance utilization and algorithm metrics\n",
    "\n",
    "It is a simple way to check how your model is \"learning\" during the training stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore a training job and download the trained model\n",
    "\n",
    "At this point, we have a trained model stored in S3. But we are interested in making some predictions with it.\n",
    "\n",
    "After you train your model, you can deploy it using Amazon SageMaker to get predictions in any of the following ways:\n",
    "- To set up a persistent endpoint to get one prediction at a time, use SageMaker hosting services.\n",
    "- To get predictions for an entire dataset, use SageMaker batch transform.\n",
    "\n",
    "But in this notebook we do not cover this feature because sometimes we are more interested in reloading our model in a new notebook to apply an evaluation method or study its parameters or gradients. So, here we are going to download the model artifacts from S3 and load them to an \"empty\" model instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach a previous training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have just trained a model using our estimator variable in this notebook execution, we can skip this step. But probably you trained your model for hours and now you need to restore your estimator variable from a previous training job. Check for the training job you want to restore the model in SageMaker console, copy the name and paste it in the next section of code. And then you call the `attach` method of the estimator object and now you can continue to work with our training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the next cell if the previous estimator.fit command was executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-12 19:13:13 Starting - Preparing the instances for training\n",
      "2020-11-12 19:13:13 Downloading - Downloading input data\n",
      "2020-11-12 19:13:13 Training - Training image download completed. Training in progress.\n",
      "2020-11-12 19:13:13 Uploading - Uploading generated training model\n",
      "2020-11-12 19:13:13 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Set the training job you want to attach to the estimator object\n",
    "# Use this option if the training job was not trained in this execution\n",
    "my_training_job_name = 'tf-transformer-single-gpu-2020-11-12-18-36-15'\n",
    "\n",
    "# In case, when the training job have been trained in this execution, we can retrive the data from the job_name variable\n",
    "#my_training_job_name = job_name\n",
    "# Attach the estimator to the selected training job\n",
    "estimator = TensorFlow.attach(my_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name where the model will be restored:  ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36\n"
     ]
    }
   ],
   "source": [
    "# Set the job_name\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print('Job name where the model will be restored: ',estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir of model data:  s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36/output/model.tar.gz\n",
      "Dir of output data:  s3://edumunozsala-ml-sagemaker/ts-enc-dec-attention\n",
      "Buck name:  edumunozsala-ml-sagemaker\n"
     ]
    }
   ],
   "source": [
    "print('Dir of model data: ',estimator.model_data)\n",
    "print('Dir of output data: ',output_data_uri)\n",
    "print('Buck name: ',bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the trained model\n",
    "\n",
    "The estimator object variable `model_data` points to the `model.tar.gz` file which contains the saved model. And the other output files from our model that we need to rebuild and tokenize or detokenize the sentences can be found in the S3 folder `output_path/output/output.tar.gz`. We can download both files and unzip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir to download traned model:  ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36/output/model.tar.gz\n",
      "Dir to download model outputs:  ts-enc-dec-attention/ts-enc-dec-attention-single-gpu-2020-12-01-10-16-36/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Set the model and the output path in S3 to download the data \n",
    "init_model_path = len('s3://')+len(bucket_name)+1\n",
    "s3_model_path=estimator.model_data[init_model_path:]\n",
    "s3_output_data=output_data_uri[init_model_path:]+'/{}/output/output.tar.gz'.format(job_name)\n",
    "print('Dir to download traned model: ', s3_model_path)\n",
    "print('Dir to download model outputs: ', s3_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dir to download the model:  trained_model\n"
     ]
    }
   ],
   "source": [
    "print('Local dir to download the model: ',trainedmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dir to download the model:  trained_model\n"
     ]
    }
   ],
   "source": [
    "print('Local dir to download the model: ',trainedmodel_path)\n",
    "sagemaker_session.download_data(trainedmodel_path,bucket_name,s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dir to download the poutputs:  output_data\n"
     ]
    }
   ],
   "source": [
    "print('Local dir to download the poutputs: ',output_data_path)\n",
    "sagemaker_session.download_data(output_data_path,bucket_name,s3_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract the information out from the model.tar.gz file return by the training job in SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.data-00000-of-00001\n",
      "checkpoint\n",
      "decoder.index\n",
      "encoder.data-00000-of-00001\n",
      "encoder.index\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf $trainedmodel_path/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the files from output.tar.gz without recreating the directory structure, all files will be extracted to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_vocab.pkl\r\n",
      "model_info.pth\r\n",
      "out_vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf $output_data_path/output.tar.gz #--strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the tensorflow model and load the model\n",
    "\n",
    "We import the `model.py` file with our model definition but we only have the weights of the model, so we need to rebuild it. The model parameters where saved during training in the `model_info.pth`, we just need to read that file and use the parameters to initiate an empty instance of the model. And then we can load the weights, `load_weights()` into that instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters {'vocab_size_enc': 16384, 'vocab_size_dec': 16384, 'embedding_dim': 128, 'gru_units': 1024, 'att_units': 128, 'batch_size': 64}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7fc34a80b8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Encoder_Decoder_Attention.serve.enc_dec_att_model import Encoder, Decoder\n",
    "\n",
    "# Read the parameters from a dictionary\n",
    "with open(model_info_file, 'rb') as f:\n",
    "    model_info = pickle.load(f)\n",
    "print('Model parameters',model_info)\n",
    "\n",
    "# Create the Encoder\n",
    "encoder = Encoder(model_info['vocab_size_enc'], model_info['embedding_dim'], \n",
    "                      model_info['gru_units'], model_info['batch_size'])\n",
    "decoder = Decoder(model_info['vocab_size_dec'], model_info['embedding_dim'], \n",
    "                      model_info['gru_units'], model_info['batch_size'])\n",
    "#Load the saved model\n",
    "# To do: Use variable to store the model name and pass it in as a hyperparameter of the estimator\n",
    "encoder.load_weights('encoder')\n",
    "decoder.load_weights('decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions\n",
    "\n",
    "And now everything is ready to make prediction with our trained model:\n",
    "- Import the `predict.py` file with the functions to make a prediction and to translate a sentence. The code was described in the original post.\n",
    "- Read the files and load the tokenizer for the input and output sentences\n",
    "- Call to `traslate` function with the model, the tokenizers, the `sos`and `eos` tokens, the sentence to translate and the max length of the output. It returns the predicted sentence detokenize, a plain text, with the translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (4.42.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (3.8.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 5.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.11.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.18.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (2.22.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (2.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (45.2.0.post20200210)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 18.2 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.10)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=1eeea3a689d00d6a15f97c89d4bd92ce7c63d62bbca77357f5aa7be67b44d3af\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "Successfully built promise\n",
      "\u001b[31mERROR: tensorflow-metadata 0.25.0 has requirement absl-py<0.11,>=0.9, but you'll have absl-py 0.11.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dataclasses, importlib-resources, googleapis-common-protos, tensorflow-metadata, promise, dill, typing-extensions, tensorflow-datasets\n",
      "Successfully installed dataclasses-0.8 dill-0.3.3 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library necessary to tokenize the sentences\n",
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serve.predict import translate\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input and output tokenizer or vocabularis used in the training. We need them to encode and decode the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parameters from a dictionary\n",
    "#model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "with open(input_vocab_file, 'rb') as f:\n",
    "    tokenizer_inputs = pickle.load(f)\n",
    "\n",
    "with open(output_vocab_file, 'rb') as f:\n",
    "    tokenizer_outputs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: you should pay for it.\n",
      "Output sentence: Deberías pagar por ello.\n"
     ]
    }
   ],
   "source": [
    "#Show some translations\n",
    "sentence = \"you should pay for it.\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(transformer,sentence,tokenizer_inputs, tokenizer_outputs,15,model_info['sos_token_input'],\n",
    "                               model_info['eos_token_input'],model_info['sos_token_output'],\n",
    "                               model_info['eos_token_output'])\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: This is a really powerful method!\n",
      "Output sentence: ¡Esto es un montón de las carreras de las ocho!\n"
     ]
    }
   ],
   "source": [
    "#Show some translations\n",
    "sentence = \"This is a really powerful method!\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(transformer,sentence,tokenizer_inputs, tokenizer_outputs,15,model_info['sos_token_input'],\n",
    "                               model_info['eos_token_input'],model_info['sos_token_output'],\n",
    "                               model_info['eos_token_output'])\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training from a checkpoint\n",
    "\n",
    "Sometimes we need to stop our training, and maybe do some research in the performance or reallocate more resources to continue with the project. But when it is done, we need to resume the training, restoring the model and the optimizer states and continue for some more epochs to achieve a final trained model with a better performance.\n",
    "\n",
    "To help with that scenario, the `checkpoint_local_path`and `checkpoint_s3_uri` estimator parameters are much relevant. The first one is the local path, inside the container, that the algorithm writes its checkpoints to. SageMaker will persist all files under this path to `checkpoint_s3_uri` continually during training. On job startup the reverse happens - data from the s3 location is downloaded to this path before the algorithm is started. If the path is unset then SageMaker assumes the checkpoints will be provided under `/opt/ml/checkpoints/`. Using this feature we can resume training from the last checkpoint (or a previous one). \n",
    "\n",
    "For this purpose, we set the model parameter `resume = True` and `fit` the estimator to execute another training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the experiment and trial created in a previous run or create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name='tf-transformer'\n",
    "# Set the trial name \n",
    "trial_name=\"{}-{}\".format(experiment_name,'single-gpu')\n",
    "trial_comp_name = 'single-gpu-training-job'\n",
    "\n",
    "tags = [{'Key': 'my-experiments', 'Value': 'transformerEngSpa1'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the experiment\n",
      "Load the trial\n"
     ]
    }
   ],
   "source": [
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print('Load the experiment')\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        experiment = Experiment.create(experiment_name=experiment_name)\n",
    "        print('Create the experiment')\n",
    "\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    trial = Trial.load(trial_name=trial_name)\n",
    "    print('Load the trial')\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name)\n",
    "        print('Create the trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configuration parameters for the experiment\n",
    "experiment_config = {'ExperimentName': experiment.experiment_name, \n",
    "                       'TrialName': trial.trial_name,\n",
    "                       'TrialComponentDisplayName': trial_comp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Estimator for a TensorFlow 2.1 model and set the parameter `--resume` to True to force the model to restore the latest checkpoint and resume training for the number of epochs selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type='ml.m5.xlarge'\n",
    "#instance_type='ml.m4.4xlarge'\n",
    "instance_type='ml.p2.xlarge'\n",
    "#instance_type='local'\n",
    "\n",
    "# Define the metrics to search for\n",
    "metric_definitions = [{'Name': 'loss', 'Regex': 'Loss ([0-9\\\\.]+)'},{'Name': 'Accuracy', 'Regex': 'Accuracy ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an estimator with the hyperparameter resume = True\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='train',\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='tf-transformer',\n",
    "                       script_mode= True,\n",
    "                       checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       metric_definitions = metric_definitions, \n",
    "                       hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'nsamples': 60000,\n",
    "                        'resume': True,\n",
    "                        'train_file': 'spa.txt',\n",
    "                        'non_breaking_in': 'nonbreaking_prefix.en',\n",
    "                        'non_breaking_out': 'nonbreaking_prefix.es'\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-transformer-single-gpu-2020-11-12-18-36-15\n"
     ]
    }
   ],
   "source": [
    "# Set the job name and show it\n",
    "job_name = '{}-{}'.format(trial_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tf-transformer-single-gpu-2020-11-12-18-36-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-12 18:36:34 Starting - Starting the training job...\n",
      "2020-11-12 18:36:39 Starting - Launching requested ML instances......\n",
      "2020-11-12 18:37:59 Starting - Preparing the instances for training.........\n",
      "2020-11-12 18:39:13 Downloading - Downloading input data......\n",
      "2020-11-12 18:40:24 Training - Downloading the training image......\n",
      "2020-11-12 18:41:37 Training - Training image download completed. Training in progress...\u001b[34m2020-11-12 18:41:43,057 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-11-12 18:41:43,563 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": true,\n",
      "        \"non_breaking_out\": \"nonbreaking_prefix.es\",\n",
      "        \"nsamples\": 60000,\n",
      "        \"train_file\": \"spa.txt\",\n",
      "        \"model_dir\": \"s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model\",\n",
      "        \"non_breaking_in\": \"nonbreaking_prefix.en\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-transformer-single-gpu-2020-11-12-18-36-15\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model\",\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":60000,\"resume\":true,\"train_file\":\"spa.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model\",\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":60000,\"resume\":true,\"train_file\":\"spa.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-transformer-single-gpu-2020-11-12-18-36-15\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--model_dir\",\"s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model\",\"--non_breaking_in\",\"nonbreaking_prefix.en\",\"--non_breaking_out\",\"nonbreaking_prefix.es\",\"--nsamples\",\"60000\",\"--resume\",\"True\",\"--train_file\",\"spa.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=true\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_OUT=nonbreaking_prefix.es\u001b[0m\n",
      "\u001b[34mSM_HP_NSAMPLES=60000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=spa.txt\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_IN=nonbreaking_prefix.en\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --epochs 5 --model_dir s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model --non_breaking_in nonbreaking_prefix.en --non_breaking_out nonbreaking_prefix.es --nsamples 60000 --resume True --train_file spa.txt\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.11.3)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=18.1.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\u001b[0m\n",
      "\u001b[34mCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_datasets) (46.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=8ee2ee7f64812eaeae88dab477913a1a102478229062751714d6baea811887ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "  Building wheel for promise (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=6a50141ea6170764a8fea8f42c9875da12c21f0528a4b72678a6a683e10860ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\u001b[0m\n",
      "\u001b[34mSuccessfully built future promise\u001b[0m\n",
      "\u001b[34mInstalling collected packages: future, dill, googleapis-common-protos, tensorflow-metadata, attrs, tqdm, importlib-resources, typing-extensions, promise, dataclasses, tensorflow-datasets\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mSuccessfully installed attrs-20.3.0 dataclasses-0.7 dill-0.3.3 future-0.18.2 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 tqdm-4.51.0 typing-extensions-3.7.4.3\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/opt/ml/model s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-12-18-36-15/model\u001b[0m\n",
      "\u001b[34mGet the train data\u001b[0m\n",
      "\u001b[34mTokenize the input and output data and create the vocabularies\u001b[0m\n",
      "\u001b[34mInput vocab:  11460\u001b[0m\n",
      "\u001b[34mOutput vocab:  9383\u001b[0m\n",
      "\u001b[34mCreating the checkpoint ...\u001b[0m\n",
      "\u001b[34mLast checkpoint restored.\u001b[0m\n",
      "\u001b[34mTraining the model ....\u001b[0m\n",
      "\u001b[34mStarting epoch 1\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 0 Loss 0.7465 Accuracy 0.3560\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 100 Loss 0.7395 Accuracy 0.3574\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 200 Loss 0.7495 Accuracy 0.3559\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 300 Loss 0.7552 Accuracy 0.3551\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 400 Loss 0.7641 Accuracy 0.3543\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 500 Loss 0.7689 Accuracy 0.3541\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 600 Loss 0.7754 Accuracy 0.3538\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 700 Loss 0.7815 Accuracy 0.3535\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 800 Loss 0.7849 Accuracy 0.3531\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 900 Loss 0.7891 Accuracy 0.3530\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 in /opt/ml/checkpoints/ckpt-9\u001b[0m\n",
      "\u001b[34mStarting epoch 2\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 0 Loss 0.7752 Accuracy 0.3616\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 100 Loss 0.6931 Accuracy 0.3643\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 200 Loss 0.7000 Accuracy 0.3629\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 300 Loss 0.7107 Accuracy 0.3621\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 400 Loss 0.7174 Accuracy 0.3610\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 500 Loss 0.7233 Accuracy 0.3607\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 600 Loss 0.7292 Accuracy 0.3600\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 700 Loss 0.7342 Accuracy 0.3596\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 800 Loss 0.7372 Accuracy 0.3593\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 900 Loss 0.7408 Accuracy 0.3589\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 in /opt/ml/checkpoints/ckpt-10\u001b[0m\n",
      "\u001b[34mStarting epoch 3\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 0 Loss 0.6146 Accuracy 0.3627\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 100 Loss 0.6685 Accuracy 0.3686\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 200 Loss 0.6670 Accuracy 0.3673\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 300 Loss 0.6770 Accuracy 0.3664\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 400 Loss 0.6802 Accuracy 0.3660\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 500 Loss 0.6862 Accuracy 0.3653\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 600 Loss 0.6914 Accuracy 0.3650\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 700 Loss 0.6965 Accuracy 0.3646\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 800 Loss 0.7007 Accuracy 0.3640\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 900 Loss 0.7036 Accuracy 0.3641\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 in /opt/ml/checkpoints/ckpt-11\u001b[0m\n",
      "\u001b[34mStarting epoch 4\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 0 Loss 0.6168 Accuracy 0.3884\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 100 Loss 0.6127 Accuracy 0.3745\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 200 Loss 0.6317 Accuracy 0.3719\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 300 Loss 0.6400 Accuracy 0.3723\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 400 Loss 0.6493 Accuracy 0.3705\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 500 Loss 0.6558 Accuracy 0.3700\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 600 Loss 0.6614 Accuracy 0.3694\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 700 Loss 0.6638 Accuracy 0.3692\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 800 Loss 0.6702 Accuracy 0.3686\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 900 Loss 0.6730 Accuracy 0.3683\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 4 in /opt/ml/checkpoints/ckpt-12\u001b[0m\n",
      "\u001b[34mStarting epoch 5\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 0 Loss 0.6213 Accuracy 0.3940\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 100 Loss 0.5858 Accuracy 0.3795\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 200 Loss 0.6030 Accuracy 0.3767\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 300 Loss 0.6117 Accuracy 0.3760\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 400 Loss 0.6194 Accuracy 0.3754\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 500 Loss 0.6277 Accuracy 0.3743\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 600 Loss 0.6322 Accuracy 0.3742\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 700 Loss 0.6369 Accuracy 0.3733\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 800 Loss 0.6421 Accuracy 0.3726\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 900 Loss 0.6467 Accuracy 0.3722\u001b[0m\n",
      "\n",
      "2020-11-12 19:13:13 Uploading - Uploading generated training model\n",
      "2020-11-12 19:13:13 Completed - Training job completed\n",
      "\u001b[34mSaving checkpoint for epoch 5 in /opt/ml/checkpoints/ckpt-13\u001b[0m\n",
      "\u001b[34mSaving the model ....\u001b[0m\n",
      "\u001b[34mSaving the model parameters\u001b[0m\n",
      "\u001b[34mSaving the dictionaries ....\u001b[0m\n",
      "\u001b[34m2020-11-12 19:13:02,312 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2020-11-12 19:13:02,312 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 2040\n",
      "Billable seconds: 2040\n"
     ]
    }
   ],
   "source": [
    "# Fit or train the model from the latest checkpoint\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name, \n",
    "              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this training job will return a new trained model, you can download to make prediction as we describe in a former section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_experiment.delete_all(action=\"--force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Referencias for experiment and trial\n",
    "https://github.com/shashankprasanna/sagemaker-training-tutorial/blob/master/sagemaker-training-tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}