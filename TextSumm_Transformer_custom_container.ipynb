{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Guide \n",
    "\n",
    "# Training a Transformer model using a custom container\n",
    "\n",
    "Some sections of this notebook has been inspired by the tutorial:\n",
    "**SML Keras Training with Amazon SageMaker**\n",
    "https://github.com/pranaychandekar/keras-sagemaker-train\n",
    "\n",
    "**Script mode training with custom conyainer from sagemaker-examples**\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/custom-training-containers/script-mode-container/notebook/script-mode-container.ipynb\n",
    "\n",
    "In this notebook we will describe the most relevant steps to start training a custom algorithm in AWS SageMaker, not using a custom container, showing how to deal with experiments and solving some of the problems when facing with custom models when using SageMaker script mode on \n",
    "\n",
    "**Problem description**\n",
    "\n",
    "Following steps will be explained:  \n",
    "1. Create an Experiment and Trial to keep track of our experiments   \n",
    "2. Load the training data to our training instance\n",
    "3. Create the scripts to train our custom model, a Transformer.\n",
    "4. Create an Estimator to train our model in a Tensorflow 2.1 container in script mode\n",
    "5. Create a metric definitions to keep track of them in SageMaker\n",
    "4. Download the trained model to make predictions\n",
    "5. Resume training using the latest checkpoint from a previous training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment and load the libraries\n",
    "\n",
    "Let's start by setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.session.Session object at 0x7fb966918908>\n",
      "arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker_session)\n",
    "print(role)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables with data location and output location in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_list_file = 'iris_train_column_list.txt'\n",
    "data_folder_name='data'\n",
    "train_filename = 'cl_Inshorts.csv'\n",
    "non_breaking_en = 'nonbreaking_prefix.en'\n",
    "\n",
    "# Set the directories for our nodel output\n",
    "trainedmodel_path = 'trained_model'\n",
    "output_data_path = 'output_data'\n",
    "# Set the name of the artifacts that our model generate (model not included) \n",
    "model_info_file = 'model_info.pth'\n",
    "input_vocab_file = 'in_vocab.pkl'\n",
    "output_vocab_file = 'out_vocab.pkl'\n",
    "# Set the absolute path of the train data \n",
    "train_file = os.path.abspath(os.path.join(data_folder_name, train_filename))\n",
    "non_breaking_en_file = os.path.abspath(os.path.join(data_folder_name, non_breaking_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the working bucket name for this project or experiment and the three locations in S3 we will deal with:\n",
    "- Training data\n",
    "- Model and Output data\n",
    "- Checkpoint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket_name = 'edumunozsala-ml-sagemaker'\n",
    "project_name = \"ts-transformer\"\n",
    "\n",
    "training_data_folder = r'{}/data'.format(project_name)\n",
    "output_folder = r'{}'.format(project_name)\n",
    "ckpt_folder = r'{}/ckpt'.format(project_name)\n",
    "\n",
    "training_data_uri = r's3://' + bucket_name + r'/' + training_data_folder\n",
    "output_data_uri = r's3://' + bucket_name + r'/' + output_folder\n",
    "ckpt_data_uri = r's3://' + bucket_name + r'/' + ckpt_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://edumunozsala-ml-sagemaker/ts-transformer/data',\n",
       " 's3://edumunozsala-ml-sagemaker/ts-transformer',\n",
       " 's3://edumunozsala-ml-sagemaker/ts-transformer/ckpt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_uri,output_data_uri,ckpt_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not yet in the S3 folder we upload it in the next code section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://edumunozsala-ml-sagemaker/ts-transformer/data/nonbreaking_prefix.en'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_data_folder)\n",
    "\n",
    "sagemaker_session.upload_data(non_breaking_en_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker-experiments\n",
      "  Using cached sagemaker_experiments-0.1.25-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker-experiments) (1.16.37)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.19.37)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.27->sagemaker-experiments) (1.25.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.27->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.37->boto3>=1.16.27->sagemaker-experiments) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.19.37)\n",
      "Installing collected packages: sagemaker-experiments\n",
      "Successfully installed sagemaker-experiments-0.1.25\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library necessary to handle experiments\n",
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries to work with Experiments in SageMaker\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name=project_name\n",
    "# Set the trial name \n",
    "trial_name=\"{}-{}\".format(experiment_name,'hp-turner')\n",
    "\n",
    "tags = [{'Key': 'my-experiments', 'Value': 'ts-transformer-hp'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or load the experiment and the trial: **Explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment  ts-transformer\n",
      "Loaded trial  ts-transformer-hp-turner\n"
     ]
    }
   ],
   "source": [
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    training_experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print('Loaded experiment ',experiment_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        training_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                      description = \"Experiment to track Transformer for Text Summarization\", \n",
    "                                      tags = tags)\n",
    "        print('Created experiment ',experiment_name)\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    single_gpu_trial = Trial.load(trial_name=trial_name)\n",
    "    print('Loaded trial ',trial_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        single_gpu_trial = Trial.create(experiment_name=experiment_name, \n",
    "                             trial_name= trial_name,\n",
    "                             tags = tags)\n",
    "        print('Created trial ',trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration definition for our experiment and trial\n",
    "trial_comp_name = 'custom-job'\n",
    "# Set the configuration parameters for the experiment\n",
    "experiment_config = {'ExperimentName': training_experiment.experiment_name, \n",
    "                       'TrialName': single_gpu_trial.trial_name,\n",
    "                       'TrialComponentDisplayName': trial_comp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and show information about the experiment and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:  ts-transformer\n",
      "Trial:  ts-transformer-hp-turner\n"
     ]
    }
   ],
   "source": [
    "#\"{}-{}\".format(trail_name, experiment_name)\n",
    "print('Experiment: ',training_experiment.experiment_name)\n",
    "# Show the trials in the experiment\n",
    "for trial in training_experiment.list_trials():\n",
    "    print('Trial: ',trial.trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training\n",
    "\n",
    "This tutorial's training script was adapted from TensorFlow's official [CNN MNIST example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py). We have modified it to handle the ``model_dir`` parameter passed in by SageMaker. This is an S3 path which can be used for data sharing during distributed training and checkpointing and/or model persistence. We have also added an argument-parsing function to handle processing training-related variables.\n",
    "\n",
    "At the end of the training job we have added a step to export the trained model to the path stored in the environment variable ``SM_MODEL_DIR``, which always points to ``/opt/ml/model``. This is critical because SageMaker uploads all the model artifacts in this folder to S3 at end of training.\n",
    "\n",
    "**output_data_dir**\n",
    "\n",
    "**checkpoint_iru**\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pygmentize 'Transformer/train/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the custom container image and register in Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='Dockerfile_Tra.gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{a}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "echo {a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MyNotebooks/Text_Summarization_Enc_Dec_Attention/Transformer\n",
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\r\n",
      "Starting docker:\t.[  OK  ]\r\n",
      "Sending build context to Docker daemon  50.69kB\r",
      "\r\n",
      "Step 1/14 : FROM tensorflow/tensorflow:latest-gpu-py3\n",
      " ---> e2a4af785bdb\n",
      "Step 2/14 : MAINTAINER edumunozsala \"edumunozsala@gmail.com\"\n",
      " ---> Using cache\n",
      " ---> a04e4b9f1400\n",
      "Step 3/14 : LABEL project=\"ts-transformer\"\n",
      " ---> Using cache\n",
      " ---> 35f90284828b\n",
      "Step 4/14 : ARG APP_HOME=/opt/program\n",
      " ---> Using cache\n",
      " ---> e447bb3a88a8\n",
      "Step 5/14 : ARG SOURCE_CODE=/opt/ml/code/\n",
      " ---> Using cache\n",
      " ---> aa3f5e876f91\n",
      "Step 6/14 : ARG PIP=pip3\n",
      " ---> Using cache\n",
      " ---> fb2ca70be3a9\n",
      "Step 7/14 : ENV PATH=\"${APP_HOME}:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 955ba24ff1d5\n",
      "Step 8/14 : RUN ${PIP} install --no-cache --upgrade         pip         setuptools\n",
      " ---> Using cache\n",
      " ---> faf969fc4cbe\n",
      "Step 9/14 : RUN ${PIP} install --no-cache --upgrade     sagemaker-training\n",
      " ---> Using cache\n",
      " ---> 5da3eeb4d918\n",
      "Step 10/14 : ADD train/requirements.txt /\n",
      " ---> Using cache\n",
      " ---> 6419a5a8df45\n",
      "Step 11/14 : RUN ${PIP} install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 41b9f89467c3\n",
      "Step 12/14 : COPY train/* ${SOURCE_CODE}\n",
      " ---> 73c0816d08f2\n",
      "Step 13/14 : WORKDIR ${SOURCE_CODE}\n",
      " ---> Running in de701f773f75\n",
      "Removing intermediate container de701f773f75\n",
      " ---> e65734f2c94c\n",
      "Step 14/14 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Running in 3bbe0e7deab2\n",
      "Removing intermediate container 3bbe0e7deab2\n",
      " ---> 89d4bde1826a\n",
      "Successfully built 89d4bde1826a\n",
      "Successfully tagged ts-transformer:latest\n",
      "The push refers to repository [223817798831.dkr.ecr.us-east-1.amazonaws.com/ts-transformer]\n",
      "5a05811596dd: Preparing\n",
      "b737c480fb7c: Preparing\n",
      "128263929bf6: Preparing\n",
      "8f41e0075060: Preparing\n",
      "bc5a90ba6bbc: Preparing\n",
      "8bdbbc38d97a: Preparing\n",
      "6e576898edb7: Preparing\n",
      "b8f75ed65728: Preparing\n",
      "2acb7acd7288: Preparing\n",
      "9e34e92e6b95: Preparing\n",
      "e24139c523b9: Preparing\n",
      "51997f5d204b: Preparing\n",
      "5c4b8cd3b09b: Preparing\n",
      "e44ff087319e: Preparing\n",
      "808fd332a58a: Preparing\n",
      "b16af11cbf29: Preparing\n",
      "37b9a4b22186: Preparing\n",
      "e0b3afb09dc3: Preparing\n",
      "6c01b5a53aac: Preparing\n",
      "2c6ac8e5063e: Preparing\n",
      "cc967c529ced: Preparing\n",
      "51997f5d204b: Waiting\n",
      "5c4b8cd3b09b: Waiting\n",
      "e44ff087319e: Waiting\n",
      "808fd332a58a: Waiting\n",
      "b16af11cbf29: Waiting\n",
      "37b9a4b22186: Waiting\n",
      "e0b3afb09dc3: Waiting\n",
      "6c01b5a53aac: Waiting\n",
      "2c6ac8e5063e: Waiting\n",
      "cc967c529ced: Waiting\n",
      "8bdbbc38d97a: Waiting\n",
      "6e576898edb7: Waiting\n",
      "b8f75ed65728: Waiting\n",
      "2acb7acd7288: Waiting\n",
      "9e34e92e6b95: Waiting\n",
      "e24139c523b9: Waiting\n",
      "bc5a90ba6bbc: Layer already exists\n",
      "128263929bf6: Layer already exists\n",
      "8f41e0075060: Layer already exists\n",
      "b737c480fb7c: Layer already exists\n",
      "8bdbbc38d97a: Layer already exists\n",
      "b8f75ed65728: Layer already exists\n",
      "6e576898edb7: Layer already exists\n",
      "2acb7acd7288: Layer already exists\n",
      "e24139c523b9: Layer already exists\n",
      "9e34e92e6b95: Layer already exists\n",
      "51997f5d204b: Layer already exists\n",
      "5c4b8cd3b09b: Layer already exists\n",
      "e44ff087319e: Layer already exists\n",
      "808fd332a58a: Layer already exists\n",
      "b16af11cbf29: Layer already exists\n",
      "37b9a4b22186: Layer already exists\n",
      "e0b3afb09dc3: Layer already exists\n",
      "6c01b5a53aac: Layer already exists\n",
      "cc967c529ced: Layer already exists\n",
      "2c6ac8e5063e: Layer already exists\n",
      "5a05811596dd: Pushed\n",
      "latest: digest: sha256:c6284840e62e0de98d495865fecc918b1ccca3b9a76cc8dc52653944c46c8115 size: 4727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# Move to the folder with the Dockerfile\n",
    "cd Transformer\n",
    "pwd\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=ts-transformer\n",
    "\n",
    "chmod +x train/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "# Comment the line below to use a GPU\n",
    "#docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "\n",
    "# Uncomment the below line if you wish to run on a GPU\n",
    "docker build  -t ${algorithm_name} -f Dockerfile_Tra.gpu . \n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* `py_version` is set to `'py3'` to indicate that we are using script mode since legacy mode supports only Python 2. Though Python 2 will be deprecated soon, you can use script mode with Python 2 by setting `py_version` to `'py2'` and `script_mode` to `True`.\n",
    "\n",
    "* `distributions` is used to configure the distributed training setup. It's required only if you are doing distributed training either across a cluster of instances or across multiple GPUs. Here we are using parameter servers as the distributed training schema. SageMaker training jobs run on homogeneous clusters. To make parameter server more performant in the SageMaker setup, we run a parameter server on every instance in the cluster, so there is no need to specify the number of parameter servers to launch. Script mode also supports distributed training with [Horovod](https://github.com/horovod/horovod). You can find the full documentation on how to configure `distributions` [here](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables for account, region and container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image uri:  223817798831.dkr.ecr.us-east-1.amazonaws.com/ts-transformer\n"
     ]
    }
   ],
   "source": [
    "account = boto3.client('sts').get_caller_identity().get('Account') # aws account \n",
    "#container_image = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account, region, project_name) # algorithm image path in ECR\n",
    "container_image = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account, region, project_name) # algorithm image path in ECR\n",
    "print('container image uri: ',container_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also initiate an estimator to train with TensorFlow 2.1 script. The only things that you will need to change are the script name and ``framewotk_version``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type='ml.m5.xlarge'\n",
    "#instance_type='ml.m4.4xlarge'\n",
    "instance_type='ml.p2.xlarge'\n",
    "#instance_type='local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define use of checkpoint and resumen\n",
    "How to use, how it works\n",
    "\n",
    "The local path that the algorithm writes its checkpoints to. SageMaker will persist all files under this path to checkpoint_s3_uri continually during training. On job startup the reverse happens - data from the s3 location is downloaded to this path before the algorithm is started. If the path is unset then SageMaker assumes the checkpoints will be provided under /opt/ml/checkpoints/.\n",
    "\n",
    "- Define the use of metrics\n",
    "You can monitor the metrics that a training job emits in real time in the **CloudWatch console**\n",
    "To monitor training job metrics (CloudWatch console)\n",
    "\n",
    "Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/.\n",
    "\n",
    "Choose Metrics, then choose /aws/sagemaker/TrainingJobs.\n",
    "\n",
    "Choose TrainingJobName.\n",
    "\n",
    "On the All metrics tab, choose the names of the training metrics that you want to monitor.\n",
    "\n",
    "On the Graphed metrics tab, configure the graph options. For more information about using CloudWatch graphs, see Graph Metrics in the Amazon CloudWatch User Guide\n",
    "\n",
    "You can monitor the metrics that a training job emits in real time by using the **SageMaker console**.\n",
    "\n",
    "To monitor training job metrics (SageMaker console)\n",
    "\n",
    "Open the SageMaker console at https://console.aws.amazon.com/sagemaker/.\n",
    "\n",
    "Choose Training jobs, then choose the training job whose metrics you want to see.\n",
    "\n",
    "Choose TrainingJobName.\n",
    "\n",
    "In the Monitor section, you can review the graphs of instance utilization and algorithm metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to search for\n",
    "#metric_definitions = [{'Name': 'loss', 'Regex': 'Loss ([0-9\\\\.]+)'},{'Name': 'Accuracy', 'Regex': 'Accuracy ([0-9\\\\.]+)'}]\n",
    "# Define the metrics to search for\n",
    "metric_definitions = [{'Name': 'train_loss', 'Regex': 'Train: [0-9a-zA-Z. ]+ Loss ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_loss', 'Regex': 'Validation: [0-9a-zA-Z. ]+ Loss ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_rouge1', 'Regex': 'Validation: [0-9a-zA-Z. ]+ Rouge1 ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_rouge2', 'Regex': 'Validation: [0-9a-zA-Z. ]+ Rouge2 ([0-9\\\\.]+)'},\n",
    "                      {'Name': 'val_rougel', 'Regex': 'Validation: [0-9a-zA-Z. ]+ RougeL ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(#entry_point='train.py',\n",
    "                       #source_dir=\"train\",\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       image_uri=container_image,\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='tf-transformer',\n",
    "                       script_mode= True,\n",
    "                       #checkpoint_local_path = 'ckpt',\n",
    "                       #checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       metric_definitions = metric_definitions, \n",
    "                       hyperparameters={\n",
    "                        'epochs': 3,\n",
    "                        'nsamples': 54000,\n",
    "                        'd_model': 512,\n",
    "                        'ffn_dim': 512,\n",
    "                        'n_layers': 6,\n",
    "                        'resume': True,\n",
    "                        'train_file': 'cl_Inshorts.csv',\n",
    "                        'non_breaking_in': 'nonbreaking_prefix.en',\n",
    "                        'non_breaking_out': 'nonbreaking_prefix.en'\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling ``fit``\n",
    "\n",
    "To start a training job, we call `estimator.fit(training_data_uri)`.\n",
    "\n",
    "An S3 location is used here as the input. `fit` creates a default channel named `'training'`, which points to this S3 location. In the training script we can then access the training data from the location stored in `SM_CHANNEL_TRAINING`. `fit` accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "\n",
    "When training starts, the TensorFlow container executes mnist.py, passing `hyperparameters` and `model_dir` from the estimator as script arguments. Because we didn't define either in this example, no hyperparameters are passed, and `model_dir` defaults to `s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>`, so the script execution is as follows:\n",
    "```bash\n",
    "python mnist.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>\n",
    "```\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit to train a model with TensorFlow 2.1 scroipt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts-transformer-hp-turner-2020-12-31-19-37-07\n"
     ]
    }
   ],
   "source": [
    "#job_name=f'tensorflow-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "job_name = '{}-{}'.format(trial_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: ts-transformer-hp-turner-2020-12-31-19-37-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-31 19:37:09 Starting - Starting the training job...\n",
      "2020-12-31 19:37:32 Starting - Launching requested ML instancesProfilerReport-1609443429: InProgress\n",
      "......\n",
      "2020-12-31 19:38:33 Starting - Preparing the instances for training......\n",
      "2020-12-31 19:39:34 Downloading - Downloading input data...\n",
      "2020-12-31 19:39:54 Training - Downloading the training image...........\u001b[34m2020-12-31 19:41:50,444 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rouge_score in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.10.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets->-r requirements.txt (line 7)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 7)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 2)) (2020.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score->-r requirements.txt (line 5)) (3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score->-r requirements.txt (line 5)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 4)) (0.24.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (0.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (20.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (4.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (5.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (1.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (0.19.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (3.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (5.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r requirements.txt (line 6)) (3.1.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 6)) (4.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 6)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.4 in /usr/local/lib/python3.6/dist-packages (from importlib-resources->tensorflow_datasets->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk->rouge_score->-r requirements.txt (line 5)) (2020.11.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk->rouge_score->-r requirements.txt (line 5)) (1.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r requirements.txt (line 1)) (1.52.0)\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,571 sagemaker-training-toolkit INFO     Failed to parse hyperparameter resume value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,571 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_out value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,571 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value cl_Inshorts.csv to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,571 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_in value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,597 sagemaker-training-toolkit INFO     Failed to parse hyperparameter resume value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,597 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_out value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,597 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value cl_Inshorts.csv to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,597 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_in value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,621 sagemaker-training-toolkit INFO     Failed to parse hyperparameter resume value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,622 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_out value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,622 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value cl_Inshorts.csv to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,622 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_in value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:52,633 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": \"True\",\n",
      "        \"d_model\": 512,\n",
      "        \"non_breaking_out\": \"nonbreaking_prefix.en\",\n",
      "        \"ffn_dim\": 512,\n",
      "        \"nsamples\": 54000,\n",
      "        \"train_file\": \"cl_Inshorts.csv\",\n",
      "        \"non_breaking_in\": \"nonbreaking_prefix.en\",\n",
      "        \"epochs\": 3,\n",
      "        \"n_layers\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"ts-transformer-hp-turner-2020-12-31-19-37-07\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"d_model\":512,\"epochs\":3,\"ffn_dim\":512,\"n_layers\":6,\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.en\",\"nsamples\":54000,\"resume\":\"True\",\"train_file\":\"cl_Inshorts.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"d_model\":512,\"epochs\":3,\"ffn_dim\":512,\"n_layers\":6,\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.en\",\"nsamples\":54000,\"resume\":\"True\",\"train_file\":\"cl_Inshorts.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"ts-transformer-hp-turner-2020-12-31-19-37-07\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--d_model\",\"512\",\"--epochs\",\"3\",\"--ffn_dim\",\"512\",\"--n_layers\",\"6\",\"--non_breaking_in\",\"nonbreaking_prefix.en\",\"--non_breaking_out\",\"nonbreaking_prefix.en\",\"--nsamples\",\"54000\",\"--resume\",\"True\",\"--train_file\",\"cl_Inshorts.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=True\u001b[0m\n",
      "\u001b[34mSM_HP_D_MODEL=512\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_OUT=nonbreaking_prefix.en\u001b[0m\n",
      "\u001b[34mSM_HP_FFN_DIM=512\u001b[0m\n",
      "\u001b[34mSM_HP_NSAMPLES=54000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=cl_Inshorts.csv\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_IN=nonbreaking_prefix.en\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_N_LAYERS=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --d_model 512 --epochs 3 --ffn_dim 512 --n_layers 6 --non_breaking_in nonbreaking_prefix.en --non_breaking_out nonbreaking_prefix.en --nsamples 54000 --resume True --train_file cl_Inshorts.csv\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-31 19:41:56.663322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-31 19:41:55 Training - Training image download completed. Training in progress.\u001b[34m2020-12-31 19:41:56.720561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\u001b[0m\n",
      "\u001b[34mwandb: W&B API key is configured (use `wandb login --relogin` to force relogin)\u001b[0m\n",
      "\u001b[34m/opt/ml/model None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.10.12\u001b[0m\n",
      "\u001b[34mwandb: Syncing run Transformer-Tuner\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/edumunozsala/transformer_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/edumunozsala/transformer_demo/runs/ts-transformer-hp-turner-2020-12-31-19-37-07-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20201231_194200-ts-transformer-hp-turner-2020-12-31-19-37-07-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mGet the train data\u001b[0m\n",
      "\u001b[34mToken for sos and eos: 5 6 5 6\u001b[0m\n",
      "\u001b[34mSize of Input Vocabulary:  16384\u001b[0m\n",
      "\u001b[34mSize of Output Vocabulary:  16384\u001b[0m\n",
      "\u001b[34mInput vocab:  16384\u001b[0m\n",
      "\u001b[34mOutput vocab:  16384\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.449976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.646905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.647706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \u001b[0m\n",
      "\u001b[34mpciBusID: 0000:00:1e.0 name: Tesla K80 computeCapability: 3.7\u001b[0m\n",
      "\u001b[34mcoreClock: 0.8755GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.647760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.647812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.706540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.719042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.832106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.843050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.843113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.843275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.844146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.844894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.845658: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.869447: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300080000 Hz\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.870962: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5317580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:11.871006: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.130707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.131599: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5309990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.131621: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.132821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \u001b[0m\n",
      "\u001b[34mpciBusID: 0000:00:1e.0 name: Tesla K80 computeCapability: 3.7\u001b[0m\n",
      "\u001b[34mcoreClock: 0.8755GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.133888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.134697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.135402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:12.137040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:13.185028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:13.185065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:13.185074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:13.187084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:13.188006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:13.188753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10777 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.96k [00:00<?, ?B/s]#015Downloading: 4.93kB [00:00, 3.12MB/s]                   \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCreating the checkpoint ...\u001b[0m\n",
      "\u001b[34mStarting epoch 1\u001b[0m\n",
      "\u001b[34m2020-12-31 19:42:34.622966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 0 Loss 6.8183\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 100 Loss 6.6295\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 200 Loss 6.2856\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 300 Loss 5.9428\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 400 Loss 5.6882\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 500 Loss 5.5217\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 600 Loss 5.4002\u001b[0m\n",
      "\u001b[34mTrain: Epoch 1 Batch 700 Loss 5.2961\u001b[0m\n",
      "\u001b[34mValidation: Epoch 1 Batch 125 Loss 4.8556 Rouge1 30.7389 Rouge2 3.1029 RougeL 30.5964\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 in /opt/ml/checkpoints/ckpt-1\u001b[0m\n",
      "\u001b[34mTime for 1 epoch: 798.2471005916595 secs\n",
      "\u001b[0m\n",
      "\u001b[34mStarting epoch 2\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 0 Loss 4.4056\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 100 Loss 4.4998\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 200 Loss 4.4580\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 300 Loss 4.4192\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 400 Loss 4.3822\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 500 Loss 4.3457\u001b[0m\n",
      "\u001b[34mTrain: Epoch 2 Batch 600 Loss 4.3139\u001b[0m\n",
      "\n",
      "2020-12-31 20:01:10 Stopping - Stopping the training job\u001b[34mTrain: Epoch 2 Batch 700 Loss 4.2819\u001b[0m\n",
      "\n",
      "2020-12-31 20:03:28 Uploading - Uploading generated training model\n",
      "2020-12-31 20:03:28 Stopped - Training job stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 1434\n",
      "Billable seconds: 1434\n"
     ]
    }
   ],
   "source": [
    "#estimator.fit({'training':training_data_uri,'testing':testing_data_uri})\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name, \n",
    "              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiment, then you can view it and its trials from SageMaker Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f7992651ac8>,experiment_name='ts-transformer',experiment_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment/ts-transformer',display_name='ts-transformer',description='Experiment to track Transformer for Text Summarization',creation_time=datetime.datetime(2020, 12, 28, 16, 18, 3, 736000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 12, 28, 19, 18, 20, 572000, tzinfo=tzlocal()),last_modified_by={},response_metadata={'RequestId': 'c75a1cbe-79b9-4743-9dae-6ade7785ce29', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c75a1cbe-79b9-4743-9dae-6ade7785ce29', 'content-type': 'application/x-amz-json-1.1', 'content-length': '86', 'date': 'Tue, 29 Dec 2020 19:32:39 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trial\n",
    "single_gpu_trial.save()\n",
    "# Save the experiment\n",
    "training_experiment.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show metrics from SageMaker Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show where you can see the metrics from both sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach a previous training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for your the training job you want to restore the model in SageMaker console, section Training jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the next cell if the previous estimator.fit command was executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf-transformer-single-gpu-2020-11-09-16-18-20'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DescribeTrainingJob operation: Requested resource not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ef90c3184c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmy_training_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Attach the estimator to the selected training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_training_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         estimator = super(Framework, cls).attach(\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mtraining_job_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_channel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m         )\n\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         job_details = sagemaker_session.sagemaker_client.describe_training_job(\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_job_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         )\n\u001b[1;32m    674\u001b[0m         \u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_init_params_from_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_channel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DescribeTrainingJob operation: Requested resource not found."
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Set the training job you want to attach to the estimator object\n",
    "# Use this option if the training job was not trained in this execution\n",
    "#my_training_job_name = 'single-gpu-2020-11-08-18-40-33'\n",
    "\n",
    "# In case, when the training job have been trained in this execution, we can retrive the data from the job_name variable\n",
    "my_training_job_name = job_name\n",
    "# Attach the estimator to the selected training job\n",
    "estimator = TensorFlow.attach(my_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name where the model will be restored:  tf-transformer-single-gpu-2020-11-09-16-18-20\n"
     ]
    }
   ],
   "source": [
    "print('Job name where the model will be restored: ',estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir of model data:  s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-09-16-18-20/model.tar.gz\n",
      "Dir of output data:  s3://edumunozsala-ml-sagemaker/transformer-nmt\n",
      "Buck name:  edumunozsala-ml-sagemaker\n"
     ]
    }
   ],
   "source": [
    "print('Dir of model data: ',estimator.model_data)\n",
    "print('Dir of output data: ',output_data_uri)\n",
    "print('Buck name: ',bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir to download traned model:  transformer-nmt/tf-transformer-single-gpu-2020-11-09-16-18-20/model.tar.gz\n",
      "Dir to download model outputs:  transformer-nmt/tf-transformer-single-gpu-2020-11-09-16-18-20/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#s3_model_path='transformer-nmt/tf-transformer-2020-11-07-17-49-03-516/output/model.tar.gz'\n",
    "init_model_path = len('s3://')+len(bucket_name)+1\n",
    "s3_model_path=estimator.model_data[init_model_path:]\n",
    "s3_output_data=output_data_uri[init_model_path:]+'/{}/output.tar.gz'.format(job_name)\n",
    "print('Dir to download traned model: ', s3_model_path)\n",
    "print('Dir to download model outputs: ', s3_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(trainedmodel_path,bucket_name,s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(output_data_path,bucket_name,s3_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract the information out from the model.tar.gz file return by the training job in SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.index\r\n",
      "checkpoint\r\n",
      "transformer.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf $trainedmodel_path/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the files from output.tar.gz without recreating the directory structure, all files will be extracted to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out_vocab.pkl\n",
      "data/in_vocab.pkl\n",
      "data/model_info.pth\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf $output_data_path/output.tar.gz --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the tensorflow model and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.model import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to restore the parameters of the model we have saved in order to build an instance of the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters {'vocab_size_enc': 1976, 'vocab_size_dec': 3865, 'sos_token_input': [1974], 'eos_token_input': [1975], 'sos_token_output': [3863], 'eos_token_output': [3864], 'n_layers': 4, 'd_model': 64, 'ffn_dim': 128, 'n_heads': 8, 'drop_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Read the parameters from a dictionary\n",
    "#model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "with open(model_info_file, 'rb') as f:\n",
    "    model_info = pickle.load(f)\n",
    "print('Model parameters',model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f847078cac8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an instance of the Transforer model and load the saved model to th\n",
    "transformer = Transformer(vocab_size_enc=model_info['vocab_size_enc'],\n",
    "                          vocab_size_dec=model_info['vocab_size_dec'],\n",
    "                          d_model=model_info['d_model'],\n",
    "                          n_layers=model_info['n_layers'],\n",
    "                          FFN_units=model_info['ffn_dim'],\n",
    "                          n_heads=model_info['n_heads'],\n",
    "                          dropout_rate=model_info['drop_rate'])\n",
    "\n",
    "#Load the saved model\n",
    "# Use a model_name argument to pass in on training and then apply here\n",
    "transformer.load_weights('transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (3.8.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.11.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.18.1)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (4.42.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (45.2.0.post20200210)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (2.2.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 15.8 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.10)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=32366049715199ee29d105c4b94f2af4ebe0f6531f26400b8fa8e683791c5031\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "Successfully built promise\n",
      "\u001b[31mERROR: tensorflow-metadata 0.25.0 has requirement absl-py<0.11,>=0.9, but you'll have absl-py 0.11.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dill, typing-extensions, promise, importlib-resources, googleapis-common-protos, tensorflow-metadata, dataclasses, tensorflow-datasets\n",
      "Successfully installed dataclasses-0.7 dill-0.3.3 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library necessary to tokenize the sentences\n",
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "from serve.predict import translate\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input and output tokenizer or vocabularis used in the training. We need them to encode and decode the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parameters from a dictionary\n",
    "#model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "with open(input_vocab_file, 'rb') as f:\n",
    "    tokenizer_inputs = pickle.load(f)\n",
    "\n",
    "with open(output_vocab_file, 'rb') as f:\n",
    "    tokenizer_outputs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: you should pay for it.\n",
      "Output sentence: \n"
     ]
    }
   ],
   "source": [
    "#Show some translations\n",
    "sentence = \"you should pay for it.\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(transformer,sentence,tokenizer_inputs, tokenizer_outputs,15,model_info['sos_token_input'],\n",
    "                               model_info['eos_token_input'],model_info['sos_token_output'],\n",
    "                               model_info['eos_token_output'])\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: This is a really powerful method!\n",
      "Output sentence: \n"
     ]
    }
   ],
   "source": [
    "#Show some translations\n",
    "sentence = \"This is a really powerful method!\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(transformer,sentence,tokenizer_inputs, tokenizer_outputs,15,model_info['sos_token_input'],\n",
    "                               model_info['eos_token_input'],model_info['sos_token_output'],\n",
    "                               model_info['eos_token_output'])\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the model on sagemaker we will try to save it from the transformer model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_fn = \"seq2seq_encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras model <train.model.Transformer object at 0x7f84707c7f98>, because its inputs are not defined.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: deploy_model/transformer_deploy/assets\n"
     ]
    }
   ],
   "source": [
    "deploy_model_path='deploy_model/transformer_deploy'\n",
    "tf.saved_model.save(transformer, deploy_model_path)\n",
    "#transformer.save(deploy_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.saved_model.load(deploy_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `deploy()` method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The [Using your own inference code]() document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deployed the trained TensorFlow 2.1 model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Creating model with name: tf-transformer-2020-11-09-17-29-19-495\n",
      "INFO:sagemaker:Creating endpoint with name tf-transformer-2020-11-09-17-29-19-495\n",
      "INFO:sagemaker.local.image:serving\n",
      "INFO:sagemaker.local.image:creating hosting dir in /tmp/tmpdig2owyk\n",
      "INFO:sagemaker.local.image:docker command: docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu\n",
      "INFO:sagemaker.local.image:image pulled: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-ugjn2:\n",
      "    command: serve\n",
      "    environment:\n",
      "    - SAGEMAKER_TFS_NGINX_LOGLEVEL=info\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-ugjn2\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpbtgygq3x:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpdig2owyk/docker-compose.yaml up --build --abort-on-container-exit\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b358>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b9b0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9bb70>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpdig2owyk_algo-1-ugjn2_1\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m   File \"/sagemaker/serve.py\", line 388, in <module>\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m     ServiceManager().start()\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m   File \"/sagemaker/serve.py\", line 342, in start\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m     self._create_tfs_config()\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m   File \"/sagemaker/serve.py\", line 92, in _create_tfs_config\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m     raise ValueError('no SavedModel bundles found!')\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m ValueError: no SavedModel bundles found!\n",
      "\u001b[36mtmpdig2owyk_algo-1-ugjn2_1 exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 627, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 687, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 632, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpdig2owyk/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n",
      "\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 10\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9ba58>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79198>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca792b0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 15\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0ba8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0be0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b438>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 20\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b940>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b358>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9bf60>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 25\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b2b0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b6d8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f849b2a2240>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 30\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9bfa90>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79390>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca794a8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 35\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca796a0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79860>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9bfac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 40\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b518>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9beb8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9ba20>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 45\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b320>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0da0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0f98>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 50\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0c88>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca32400>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f849b2a2470>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 55\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca797b8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79978>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca794e0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 60\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79a90>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79c50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79d68>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-0d1b68be025b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         )\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, update_endpoint)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m-> 2708\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m         )\n\u001b[1;32m   2710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, EndpointName, EndpointConfigName, Tags)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalEndpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mserving_port\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local.serving_port\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m8080\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0m_wait_for_serving_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserving_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;31m# the container is running and it passed the healthcheck status is now InService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalEndpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IN_SERVICE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36m_wait_for_serving_container\u001b[0;34m(serving_port)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training from a checkpoint\n",
    "\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name='tf-transformer'\n",
    "trial_name='single-gpu'\n",
    "trial_comp_name = 'single-gpu-training-job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the experiment\n",
      "Load the trial\n"
     ]
    }
   ],
   "source": [
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print('Load the experiment')\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        experiment = Experiment.create(experiment_name=experiment_name)\n",
    "        print('Create the experiment')\n",
    "\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    trial = Trial.load(trial_name=trial_name)\n",
    "    print('Load the trial')\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name)\n",
    "        print('Create the trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configuration parameters for the experiment\n",
    "experiment_config = {'ExperimentName': experiment.experiment_name, \n",
    "                       'TrialName': trial.trial_name,\n",
    "                       'TrialComponentDisplayName': trial_comp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Estimator for a TensorFlow 2.1 model and set the parameter `--resume` to True to force the model to restore the latest checkpoint and resume training for the number of epochs selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type='ml.m5.xlarge'\n",
    "#instance_type='ml.m4.4xlarge'\n",
    "instance_type='ml.p2.xlarge'\n",
    "#instance_type='local'\n",
    "\n",
    "# Define the metrics to search for\n",
    "metric_definitions = [{'Name': 'loss', 'Regex': 'Loss ([0-9\\\\.]+)'},{'Name': 'Accuracy', 'Regex': 'Accuracy ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir=\"train\",\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='tf-transformer',\n",
    "                       script_mode= True,\n",
    "                       checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       metric_definitions = metric_definitions, \n",
    "                       hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'nsamples': 40000,\n",
    "                        'resume': True,\n",
    "                        'train_file': 'spa.txt',\n",
    "                        'non_breaking_in': 'nonbreaking_prefix.en',\n",
    "                        'non_breaking_out': 'nonbreaking_prefix.es'\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single-gpu-2020-11-08-18-40-33\n"
     ]
    }
   ],
   "source": [
    "#job_name=f'tensorflow-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "job_name = '{}-{}'.format(trial_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: single-gpu-2020-11-08-18-40-33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-08 18:40:47 Starting - Starting the training job...\n",
      "2020-11-08 18:41:16 Starting - Launching requested ML instances.........\n",
      "2020-11-08 18:42:47 Starting - Preparing the instances for training.........\n",
      "2020-11-08 18:44:02 Downloading - Downloading input data...\n",
      "2020-11-08 18:44:28 Training - Downloading the training image........\u001b[34m2020-11-08 18:45:59,732 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-11-08 18:46:00,300 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": true,\n",
      "        \"non_breaking_out\": \"nonbreaking_prefix.es\",\n",
      "        \"nsamples\": 40000,\n",
      "        \"train_file\": \"spa.txt\",\n",
      "        \"model_dir\": \"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\n",
      "        \"non_breaking_in\": \"nonbreaking_prefix.en\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"single-gpu-2020-11-08-18-40-33\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":40000,\"resume\":true,\"train_file\":\"spa.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":40000,\"resume\":true,\"train_file\":\"spa.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"single-gpu-2020-11-08-18-40-33\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--model_dir\",\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\"--non_breaking_in\",\"nonbreaking_prefix.en\",\"--non_breaking_out\",\"nonbreaking_prefix.es\",\"--nsamples\",\"40000\",\"--resume\",\"True\",\"--train_file\",\"spa.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=true\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_OUT=nonbreaking_prefix.es\u001b[0m\n",
      "\u001b[34mSM_HP_NSAMPLES=40000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=spa.txt\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_IN=nonbreaking_prefix.en\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --epochs 5 --model_dir s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model --non_breaking_in nonbreaking_prefix.en --non_breaking_out nonbreaking_prefix.es --nsamples 40000 --resume True --train_file spa.txt\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.11.3)\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=18.1.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.1)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_datasets) (46.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: promise, future\n",
      "  Building wheel for promise (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-08 18:45:54 Training - Training image download completed. Training in progress.\u001b[34m  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=0e4a1bcae031239e48de28b80da81a42374fe6a78c4f4fe531599aada4269554\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=ddac4fdd0607303ad63138a91b4c623df4d1244fbac32d172ab477dfaedf8e65\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\u001b[0m\n",
      "\u001b[34mSuccessfully built promise future\u001b[0m\n",
      "\u001b[34mInstalling collected packages: dill, dataclasses, importlib-resources, googleapis-common-protos, tensorflow-metadata, attrs, tqdm, promise, typing-extensions, future, tensorflow-datasets\u001b[0m\n",
      "\u001b[34mSuccessfully installed attrs-20.3.0 dataclasses-0.7 dill-0.3.3 future-0.18.2 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 tqdm-4.51.0 typing-extensions-3.7.4.3\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/opt/ml/model s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\u001b[0m\n",
      "\u001b[34mGet the train data\u001b[0m\n",
      "\u001b[34mTokenize the input and output data and create the vocabularies\u001b[0m\n",
      "\u001b[34mInput vocab:  8596\u001b[0m\n",
      "\u001b[34mOutput vocab:  9417\u001b[0m\n",
      "\u001b[34mCreating the checkpoint ...\u001b[0m\n",
      "\u001b[34mLast checkpoint restored.\u001b[0m\n",
      "\u001b[34mTraining the model ....\u001b[0m\n",
      "\u001b[34mStarting epoch 1\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 0 Loss 1.3094 Accuracy 0.2690\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 100 Loss 1.1614 Accuracy 0.2596\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 200 Loss 1.1689 Accuracy 0.2606\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 300 Loss 1.1686 Accuracy 0.2608\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 400 Loss 1.1670 Accuracy 0.2621\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 500 Loss 1.1663 Accuracy 0.2636\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 600 Loss 1.1635 Accuracy 0.2645\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 in /opt/ml/checkpoints/ckpt-6\u001b[0m\n",
      "\u001b[34mStarting epoch 2\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 0 Loss 1.1658 Accuracy 0.2723\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 100 Loss 1.0105 Accuracy 0.2810\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 200 Loss 1.0265 Accuracy 0.2800\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 300 Loss 1.0315 Accuracy 0.2805\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 400 Loss 1.0334 Accuracy 0.2809\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 500 Loss 1.0364 Accuracy 0.2813\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 600 Loss 1.0383 Accuracy 0.2818\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 in /opt/ml/checkpoints/ckpt-7\u001b[0m\n",
      "\u001b[34mStarting epoch 3\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 0 Loss 0.9556 Accuracy 0.3013\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 100 Loss 0.8900 Accuracy 0.2980\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 200 Loss 0.9018 Accuracy 0.2975\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 300 Loss 0.9108 Accuracy 0.2974\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 400 Loss 0.9162 Accuracy 0.2974\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 500 Loss 0.9190 Accuracy 0.2972\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 600 Loss 0.9198 Accuracy 0.2976\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 in /opt/ml/checkpoints/ckpt-8\u001b[0m\n",
      "\u001b[34mStarting epoch 4\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 0 Loss 0.8783 Accuracy 0.3259\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 100 Loss 0.7916 Accuracy 0.3111\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 200 Loss 0.8035 Accuracy 0.3100\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 300 Loss 0.8117 Accuracy 0.3101\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 400 Loss 0.8199 Accuracy 0.3103\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 500 Loss 0.8245 Accuracy 0.3101\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 600 Loss 0.8283 Accuracy 0.3097\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 4 in /opt/ml/checkpoints/ckpt-9\u001b[0m\n",
      "\u001b[34mStarting epoch 5\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 0 Loss 0.7558 Accuracy 0.3382\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 100 Loss 0.7098 Accuracy 0.3224\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 200 Loss 0.7211 Accuracy 0.3213\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 300 Loss 0.7341 Accuracy 0.3210\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 400 Loss 0.7460 Accuracy 0.3201\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 500 Loss 0.7556 Accuracy 0.3191\u001b[0m\n",
      "\n",
      "2020-11-08 19:04:29 Uploading - Uploading generated training model\u001b[34mEpoch 5 Batch 600 Loss 0.7605 Accuracy 0.3186\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 5 in /opt/ml/checkpoints/ckpt-10\u001b[0m\n",
      "\u001b[34mSaving the model ....\u001b[0m\n",
      "\u001b[34mSaving the model parameters\u001b[0m\n",
      "\u001b[34mSaving the dictionaries ....\u001b[0m\n",
      "\u001b[34m2020-11-08 19:04:28,823 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2020-11-08 19:04:28,823 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-11-08 19:04:37 Completed - Training job completed\n",
      "Training seconds: 1235\n",
      "Billable seconds: 1235\n"
     ]
    }
   ],
   "source": [
    "# Fit or train the model from the latest checkpoint\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name, \n",
    "              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.00026031278, 0.990563631, 0.00917605218],\n",
       " [0.999760091, 0.000239968373, 3.97464422e-10],\n",
       " [0.000185193509, 0.974752605, 0.0250621513],\n",
       " [9.90088935e-08, 0.241644651, 0.758355319],\n",
       " [1.86230598e-09, 0.0252015758, 0.974798381]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_experiment.delete_all(action=\"--force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the TensorFlow 2.1 endpoint as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model using artifacts\n",
    "https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploy-to-a-sagemaker-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "instance_type='ml.m4.4xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/output/model.tar.gz'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The class sagemaker.tensorflow.serving.Model has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Creating model with name: tensorflow-inference-2020-11-08-19-26-16-925\n",
      "INFO:sagemaker:Creating endpoint with name tensorflow-inference-2020-11-08-19-26-17-266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint tensorflow-inference-2020-11-08-19-26-17-266: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8caaeacd937d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframework_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2.1.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, update_endpoint)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2709\u001b[0m         )\n\u001b[1;32m   2710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2978\u001b[0m                 ),\n\u001b[1;32m   2979\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             )\n\u001b[1;32m   2982\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint tensorflow-inference-2020-11-08-19-26-17-266: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "model = Model(model_data=model_data, role=role,framework_version='2.1.0')\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8, 2.7, 4.1, 1. ],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-oufpd_1  |\u001b[0m 172.18.0.1 - - [28/Mar/2020:21:15:01 +0000] \"POST /invocations HTTP/1.1\" 200 254 \"-\" \"-\"\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.00026031278, 0.990563631, 0.00917605218],\n",
       "  [0.999760091, 0.000239968373, 3.97464422e-10],\n",
       "  [0.000185193509, 0.974752605, 0.0250621513],\n",
       "  [9.90088935e-08, 0.241644651, 0.758355319],\n",
       "  [1.86230598e-09, 0.0252015758, 0.974798381]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Referencias for experiment and trial\n",
    "https://github.com/shashankprasanna/sagemaker-training-tutorial/blob/master/sagemaker-training-tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
