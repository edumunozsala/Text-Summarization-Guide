{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization using machine learning techniques\n",
    "\n",
    "The purpose of this notebook is to create a set of functions with some techniques frequently used in NLP tasks to clean and prepare the text data. Given a CSV file containing our source text and summaries we will transform them to a well-suited text data to train our text summarization models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils and text processing libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import os, shutil\n",
    "import unicodedata\n",
    "\n",
    "# Import library to split our dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the wotrking dir\n",
    "#!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove the punctuations but we will keep the \".\" punctuation wich is necessary to work with sentences as a delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "punctuation = string.punctuation.replace('.', '')\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data path and filename of our dataset\n",
    "DATADIR='data/'\n",
    "data_filename= 'Inshorts Cleaned Data.xlsx'\n",
    "datafile_type='xls'\n",
    "# Depending on the datafile we set the names of the columns to use\n",
    "cols_to_use = ['Headline','Short']\n",
    "# Set the enriment variabe DATA if we need to copy the file from GCS\n",
    "os.environ['DATA'] = DATADIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We must run the next cell the very first time or when the datafile has been modified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only when we need to create or update the dataset file\n",
    "#Create an enviroment variable wuth the data path and \n",
    "#shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "#os.makedirs(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://mlend_text_summarization/data/news_summary/news_summary.csv...\n",
      "/ [1 files][ 11.4 MiB/ 11.4 MiB]                                                \n",
      "Operation completed over 1 objects/11.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp gs://mlend_text_summarization/data/news_summary/news_summary.csv ${DATA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset with the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  55104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Short</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>Publish Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
       "      <td>The CBI on Saturday booked four former officia...</td>\n",
       "      <td>The New Indian Express</td>\n",
       "      <td>09:25:00</td>\n",
       "      <td>2017-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
       "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
       "      <td>At least three people were killed, including a...</td>\n",
       "      <td>Hindustan Times</td>\n",
       "      <td>23:39:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why has Reliance been barred from trading in f...</td>\n",
       "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>23:08:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Was stopped from entering my own studio at Tim...</td>\n",
       "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>23:24:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  4 ex-bank officials booked for cheating bank o...   \n",
       "1     Supreme Court to go paperless in 6 months: CJI   \n",
       "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
       "3  Why has Reliance been barred from trading in f...   \n",
       "4  Was stopped from entering my own studio at Tim...   \n",
       "\n",
       "                                               Short                 Source   \\\n",
       "0  The CBI on Saturday booked four former officia...  The New Indian Express   \n",
       "1  Chief Justice JS Khehar has said the Supreme C...                 Outlook   \n",
       "2  At least three people were killed, including a...         Hindustan Times   \n",
       "3  Mukesh Ambani-led Reliance Industries (RIL) wa...                Livemint   \n",
       "4  TV news anchor Arnab Goswami has said he was t...                 YouTube   \n",
       "\n",
       "      Time  Publish Date  \n",
       "0  09:25:00   2017-03-26  \n",
       "1  22:18:00   2017-03-25  \n",
       "2  23:39:00   2017-03-25  \n",
       "3  23:08:00   2017-03-25  \n",
       "4  23:24:00   2017-03-25  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file to a dataframe\n",
    "if datafile_type == 'csv':\n",
    "    data = pd.read_csv(DATADIR + data_filename, encoding='utf-8')\n",
    "elif datafile_type == 'xls':\n",
    "    # Load the data from the file system\n",
    "    data = pd.read_excel(DATADIR + data_filename, encoding='utf-8')\n",
    "\n",
    "print(\"Number of examples: \", len(data))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Censor Board demands removal of phrase Mann Ki Baat in film \n",
      " Censor Board has demanded the removal of phrase &#39;Mann Ki Baat&#39; from a dialogue in film &#39;Sameer&#39;, as it is also the title of PM Narendra Modi&#39;s radio show. The Board hasn&#39;t objected to an expletive in the same dialogue. &#34;The board, despite granting us an A certificate, asked for certain scenes to be chopped mercilessly,&#34; said the film&#39;s director. \n",
      " India Today\n"
     ]
    }
   ],
   "source": [
    "print(data['Headline'][50],'\\n',data['Short'][50],'\\n',data['Source '][50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L&amp;T Q1 net profit up 46% to ₹610 cr\n"
     ]
    }
   ],
   "source": [
    "print(data['Headline'][38703])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L&T Q1 net profit up 46% to e610 cr\n"
     ]
    }
   ],
   "source": [
    "s=replace_mapping(data['Headline'][38703], special_mapping)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the examples with null values or duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Short</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>Publish Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54997</td>\n",
       "      <td>54997</td>\n",
       "      <td>54997</td>\n",
       "      <td>54997</td>\n",
       "      <td>54997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>54939</td>\n",
       "      <td>54997</td>\n",
       "      <td>1471</td>\n",
       "      <td>1405</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sensex, Nifty end on a flat note</td>\n",
       "      <td>Former Pakistan President Pervez Musharraf has...</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>2016-10-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4685</td>\n",
       "      <td>127</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-26 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Headline  \\\n",
       "count                              54997   \n",
       "unique                             54939   \n",
       "top     Sensex, Nifty end on a flat note   \n",
       "freq                                   9   \n",
       "first                                NaN   \n",
       "last                                 NaN   \n",
       "\n",
       "                                                    Short  Source      Time   \\\n",
       "count                                               54997    54997     54997   \n",
       "unique                                              54997     1471      1405   \n",
       "top     Former Pakistan President Pervez Musharraf has...  YouTube  10:00:00   \n",
       "freq                                                    1     4685       127   \n",
       "first                                                 NaN      NaN       NaN   \n",
       "last                                                  NaN      NaN       NaN   \n",
       "\n",
       "               Publish Date  \n",
       "count                 54997  \n",
       "unique                  433  \n",
       "top     2016-10-11 00:00:00  \n",
       "freq                    234  \n",
       "first   2016-01-19 00:00:00  \n",
       "last    2017-03-26 00:00:00  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove duplicates on the ctext \n",
    "data.drop_duplicates(subset=[\"Short\"],inplace=True)\n",
    "#Remove rows containing null values\n",
    "data.dropna(inplace=True)\n",
    "#Recreate the dataframe index\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54997 entries, 0 to 54996\n",
      "Data columns (total 5 columns):\n",
      "Headline        54997 non-null object\n",
      "Short           54997 non-null object\n",
      "Source          54997 non-null object\n",
      "Time            54997 non-null object\n",
      "Publish Date    54997 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows, null values, etc\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove all the columns that are not needed in our exercise. We only save the xtext variable and the text variable. They become the text variable and summary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             summary  \\\n",
      "0  4 ex-bank officials booked for cheating bank o...   \n",
      "1     Supreme Court to go paperless in 6 months: CJI   \n",
      "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
      "3  Why has Reliance been barred from trading in f...   \n",
      "4  Was stopped from entering my own studio at Tim...   \n",
      "\n",
      "                                                text  \n",
      "0  The CBI on Saturday booked four former officia...  \n",
      "1  Chief Justice JS Khehar has said the Supreme C...  \n",
      "2  At least three people were killed, including a...  \n",
      "3  Mukesh Ambani-led Reliance Industries (RIL) wa...  \n",
      "4  TV news anchor Arnab Goswami has said he was t...  \n"
     ]
    }
   ],
   "source": [
    "# we are using the text variable as the summary and the ctext as the source text\n",
    "dataset = data[cols_to_use].copy()\n",
    "dataset.columns = ['summary','text']\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Censor Board demands removal of phrase Mann Ki Baat in film \n",
      " Censor Board has demanded the removal of phrase &#39;Mann Ki Baat&#39; from a dialogue in film &#39;Sameer&#39;, as it is also the title of PM Narendra Modi&#39;s radio show. The Board hasn&#39;t objected to an expletive in the same dialogue. &#34;The board, despite granting us an A certificate, asked for certain scenes to be chopped mercilessly,&#34; said the film&#39;s director. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['summary'][50],'\\n',dataset['text'][50],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanings and preprocess\n",
    "\n",
    "Now it is time to apply some functions for common text cleaning operations like:\n",
    "- Remove URLs\n",
    "- Remove html tags\n",
    "- Remove some emojis\n",
    "- Expand common contractions\n",
    "- Expand some Slang abbrevation\n",
    "- Remove punctuation\n",
    "- Remove non-character (Unicode \\xFF)\n",
    "- Remove break line \\n\n",
    "- Remove &amp\n",
    "- Remove mention @\n",
    "- Remove hastag #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping to handle contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def expand_contractions(text, mapping):\n",
    "    ''' Expand the contractions (some well-known of them) in a text'''\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \n",
    "                 \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', \n",
    "                 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', 'Â':'', 'Ł':'',\n",
    "                'Ă': '', '&#39;':\"'\", '&#34;':'\"', '&amp;':'&'}\n",
    "\n",
    "def remove_URL(text):\n",
    "    ''' Remove URLs from the text'''\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "# Remove html tag\n",
    "def remove_html(text):\n",
    "    ''' Remove HTML tags from the text'''\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_mention(text):\n",
    "    ''' Remove mentions from the text'''\n",
    "    url = re.compile(r'@\\S*')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_mult_spaces(text):\n",
    "    ''' Reduce multispace to one single space'''\n",
    "    re_mult_space = re.compile(r\"  *\") # replace multiple spaces with just one\n",
    "    return re_mult_space.sub(r' ', text)\n",
    "\n",
    "def remove_non_character(text):\n",
    "    ''' Remove some non alphanumeric characters'''\n",
    "    url = re.compile(r'\\x89\\S*|\\x9b\\S*|\\x92\\S*|x93\\S*|\\x8a\\S*|\\x8f\\S*|\\x9d\\S*|\\x8c\\S*|\\x91\\S*|\\x87\\S*|\\x88\\S*|\\x82\\S*')\n",
    "    #url = re.compile(r'\\x\\d+\\S*')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_punctuation(text, punctuation):\n",
    "    ''' Remove punctuation from the text'''\n",
    "    table=str.maketrans('','',punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    ''' Remove emojis from the text'''\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_CTL(text):\n",
    "    ''' Remove end of line from the text'''\n",
    "    url = re.compile(r'\\n')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    ''' Remove hashtags from the text'''\n",
    "    url = re.compile(r'#\\S*')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def replace_mapping(text, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that will call all the others cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' Clean the input text using common techniques'''\n",
    "    new_text = text\n",
    "    #new_text=new_text.apply(lambda x : unicode_to_ascii(x))\n",
    "    new_text=new_text.apply(lambda x : replace_mapping(x, special_mapping))\n",
    "    new_text=new_text.apply(lambda x : remove_URL(x))\n",
    "    new_text=new_text.apply(lambda x : remove_html(x))\n",
    "    new_text=new_text.apply(lambda x : remove_emoji(x))\n",
    "    new_text=new_text.apply(lambda x : expand_contractions(x,contraction_mapping))\n",
    "    new_text=new_text.apply(lambda x : remove_non_character(x))\n",
    "    new_text=new_text.apply(lambda x : remove_CTL(x))\n",
    "    new_text=new_text.apply(lambda x : remove_mention(x))\n",
    "    new_text=new_text.apply(lambda x : remove_hashtag(x))\n",
    "    #new_text=new_text.apply(lambda x : remove_punctuation(x, punctuation))\n",
    "    new_text=new_text.apply(lambda x : remove_mult_spaces(x))\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and apply more complex preprocessing task\n",
    "Next step, Tokenize or split our text into tokens or words. *Tokenization: Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence*. These tokens will be our working unit. Then we can apply some preprocessing steps depending on the task: \n",
    "\n",
    "   - Lowercase\n",
    "   - Remove very short tokens\n",
    "   - Remove Stopwors\n",
    "   - Steeming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to apply to the cleaing process\n",
    "\n",
    "# If stopwords_file is defined then we take the stop words list from this file, if not we use the stopwords from NLTK\n",
    "stopw= False\n",
    "#stopwords_file='NLP_short_stopwords.txt'\n",
    "stopwords_file=None\n",
    "# Transform to lowercase\n",
    "lowercase = True\n",
    "# Set min length of a token\n",
    "min_length = 0\n",
    "# Set if we want to apply a lemmatizer or stemmer\n",
    "lemmatize = False\n",
    "stemming=False\n",
    "# Remove punctuation\n",
    "del_punct = False\n",
    "# Remove digits\n",
    "del_digits = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words . The general strategy for determining a stop list is to sort the terms by collection frequency (the total number of times each term appears in the document collection), and then to take the most frequent terms.\n",
    "\n",
    "Define our own stopword list or if not we can use the NLTK stopwords for english. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edumu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "if stopwords_file!=None:\n",
    "    stop_words = set(w.rstrip() for w in open(stopwords_file))\n",
    "else:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    ''' Remove stopwords from the text'''\n",
    "    w=set(text.split())-stop_words\n",
    "    return ' '.join(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer and Stemmer\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "We will use a WordNetLemmatizer and a PorterStemmer, both are frequent options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Lemmatizer object\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the Porter Stemmer object\n",
    "from nltk.stem import PorterStemmer\n",
    "wordnet_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    ''' Apply the tokenization method to the input string s.\n",
    "        We will apply, based on the setup, lowercase, remove punctuation,remove digits, \n",
    "        remove stopwords, apply stemming, apply lemmatization and remove tokens with ocurrencies\n",
    "        below a minimum defined.\n",
    "    '''\n",
    "    if lowercase:\n",
    "        s = s.lower() # downcase\n",
    "    if del_punct:\n",
    "        s=remove_punctuation(s, punctuation) # Remove punctuation\n",
    "\n",
    "    if del_digits:\n",
    "        s=remove_digits(s)  # Remove digits\n",
    "\n",
    "    tokens = word_tokenize(s) # split string into words (tokens)\n",
    "\n",
    "    tokens = [t for t in tokens if len(t) > min_length] # remove short words, they're probably not useful\n",
    "\n",
    "    if stopw:\n",
    "        tokens = [t for t in tokens if t not in stop_words] # remove stopwords\n",
    "\n",
    "    if stemming:\n",
    "        tokens = [wordnet_stemmer.stem(t) for t in tokens]  # Apply stemming \n",
    "        \n",
    "    if lemmatize:\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # Lemmatize, put words into base form\n",
    "        \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54997, 54997)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['text']),len(dataset['summary'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the cleaning process on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning functions to the source text and summary\n",
    "data_text = clean_text(dataset['text'])\n",
    "data_headlines = clean_text(dataset['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54997, 54997)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the results\n",
    "len(data_text), len(data_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Censor Board has demanded the removal of phrase \\'Mann Ki Baat\\' from a dialogue in film \\'Sameer\\', as it is also the title of PM Narendra Modi\\'s radio show. The Board has not objected to an expletive in the same dialogue. \"The board, despite granting us an A certificate, asked for certain scenes to be chopped mercilessly,\" said the film\\'s director.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and tokenize text in the dataframe\n",
    "dataset['text']=data_text\n",
    "dataset['summary']=data_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning based on the tokenizer defined and parameters\n",
    "def tokenize_and_string(sentences):\n",
    "    ''' Tokenize the sentences, apply more cleaning steps\n",
    "        Input: \n",
    "        - sentences: list of string, sentences of the text\n",
    "        Output:\n",
    "        - sentence2token: list of tokens or words, the tokens in the ever sentence\n",
    "        - token2sent: list of string, the list of sentences\n",
    "    '''\n",
    "    # Tokenize and clean every text \n",
    "    sentence2token = [my_tokenizer(sent) for sent in sentences]\n",
    "    # Convert tokens to a string\n",
    "    token2sent= [' '.join(sent) for sent in sentence2token]\n",
    "    \n",
    "    return sentence2token,token2sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text in our dataset\n",
    "_,cleaned_text=tokenize_and_string(data_text)\n",
    "_,cleaned_headlines=tokenize_and_string(data_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show some examples with our original text and the cleaned one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Censor Board has demanded the removal of phrase &#39;Mann Ki Baat&#39; from a dialogue in film &#39;Sameer&#39;, as it is also the title of PM Narendra Modi&#39;s radio show. The Board hasn&#39;t objected to an expletive in the same dialogue. &#34;The board, despite granting us an A certificate, asked for certain scenes to be chopped mercilessly,&#34; said the film&#39;s director.',\n",
       " \"censor board has demanded the removal of phrase 'mann ki baat ' from a dialogue in film 'sameer ' , as it is also the title of pm narendra modi 's radio show . the board has not objected to an expletive in the same dialogue . & board , despite granting us an a certificate , asked for certain scenes to be chopped mercilessly , & said the film 's director .\")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][50], cleaned_text[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and tokenize text in the dataframe\n",
    "dataset['text']=cleaned_text\n",
    "dataset['summary']=cleaned_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data preprocessed and cleaned\n",
    "\n",
    "We need to create a file for training our model and a different file for evaluation purpouses. This is very important in machine learning to simulate as much as possible the production enviroment were new and unseen examples will be received or feeded to our algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traning dataset will contain 85% of the examples and the remaning will be included in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  46747\n"
     ]
    }
   ],
   "source": [
    "# Set the percentage of rows to keep in the training dataset\n",
    "train_pct=0.85\n",
    "# Shuffle the whole dataframe\n",
    "dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#Define the size of the training dataset\n",
    "training_size = int(len(data)*train_pct)\n",
    "print('Training size: ', training_size)\n",
    "\n",
    "# Create a file with the whole dataset cleaned\n",
    "dataset.to_csv(DATADIR +'cl_'+data_filename, encoding='utf-8', index=False)\n",
    "#Create a file with the training dataset\n",
    "dataset.iloc[:training_size,:].to_csv(DATADIR +'cl_train_'+data_filename, encoding='utf-8',index=False)\n",
    "#Create a file with the test dataset\n",
    "dataset.iloc[training_size:,:].to_csv(DATADIR +'cl_valid_'+data_filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train and validation dataset to GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: data/cl_news_summary_train.csv\n",
      "CommandException: No URLs matched: data/cl_news_summary_valid.csv\n",
      "CommandException: No URLs matched: data/cl_news_summary_more.csv\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'gsutil cp data/cl_news_summary_train.csv gs://mlend_text_summarization/data/news_summary/\\ngsutil cp data/cl_news_summary_valid.csv gs://mlend_text_summarization/data/news_summary/\\ngsutil cp data/cl_news_summary_more.csv gs://mlend_text_summarization/data/news_summary/\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-2bdeaf896dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gsutil cp data/cl_news_summary_train.csv gs://mlend_text_summarization/data/news_summary/\\ngsutil cp data/cl_news_summary_valid.csv gs://mlend_text_summarization/data/news_summary/\\ngsutil cp data/cl_news_summary_more.csv gs://mlend_text_summarization/data/news_summary/\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'gsutil cp data/cl_news_summary_train.csv gs://mlend_text_summarization/data/news_summary/\\ngsutil cp data/cl_news_summary_valid.csv gs://mlend_text_summarization/data/news_summary/\\ngsutil cp data/cl_news_summary_more.csv gs://mlend_text_summarization/data/news_summary/\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp data/cl_news_summary_train.csv gs://mlend_text_summarization/data/news_summary/\n",
    "gsutil cp data/cl_news_summary_valid.csv gs://mlend_text_summarization/data/news_summary/\n",
    "gsutil cp data/cl_news_summary_more.csv gs://mlend_text_summarization/data/news_summary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  83606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paytm raises $1.4 billion from SoftBank in lar...</td>\n",
       "      <td>Digital payments startup Paytm has raised $1.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Petrol price cut by â¹1.12 per litre as daily...</td>\n",
       "      <td>Oil companies on Thursday reduced the petrol p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Army plans to deploy women officers for cyber ...</td>\n",
       "      <td>The Indian Army has announced plans to deploy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uday Chopra confirms YRF will produce Jessica ...</td>\n",
       "      <td>Yash Raj Films CEO Uday Chopra has confirmed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mulayam Yadav to contest 2019 polls from Mainp...</td>\n",
       "      <td>Senior Samajwadi Party leader Ram Gopal Yadav ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am so excited to play under Virat Kohli's ca...</td>\n",
       "      <td>Batsman Shubman Gill, who has been included in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Twitter reacts to women-only screening of Wond...</td>\n",
       "      <td>Reacting to Texas' The Alamo Drafthouse theatr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apple to hire engineer with psychology backgro...</td>\n",
       "      <td>Apple is hiring a software engineer with psych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Railway Board Chairman resigns after 2 derailm...</td>\n",
       "      <td>The Chairman of the Railway Board, Ashok Mitta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who is billionaire Radhakishan Damani?</td>\n",
       "      <td>Radhakishan Damani is the 61-year-old billiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Flyers told to jump onto tarmac after threaten...</td>\n",
       "      <td>Passengers were told to \"jump\" onto the tarmac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>My mother asked me how could I do this to Hard...</td>\n",
       "      <td>After Hardik Pandya and KL Rahul were suspende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4 years since Beckham announced his retirement...</td>\n",
       "      <td>Former England captain David Beckham announced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We've gone through so many CTOs, it's a joke: ...</td>\n",
       "      <td>When asked about the difficulties faced while ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>People in the film industry call me a bull: Ar...</td>\n",
       "      <td>Actor Arjun Rampal has said that he feels like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Govt proposes airing public interest messages ...</td>\n",
       "      <td>The Information and Broadcasting (I&amp;B) Ministr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Maybe true: 'Taarak Mehta Ka...' producer on D...</td>\n",
       "      <td>'Taarak Mehta Ka Ooltah Chashmah' producer Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Which teams have qualified for 2018 FIFA World...</td>\n",
       "      <td>France, who are among the six European teams t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Karnataka govt plans 1 lakh houses for poor in...</td>\n",
       "      <td>Karnataka Chief Minister Siddaramaiah will soo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pak SC bars Nawaz Sharif from holding public o...</td>\n",
       "      <td>The Pakistan Supreme Court on Friday disqualif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headlines  \\\n",
       "0   Paytm raises $1.4 billion from SoftBank in lar...   \n",
       "1   Petrol price cut by â¹1.12 per litre as daily...   \n",
       "2   Army plans to deploy women officers for cyber ...   \n",
       "3   Uday Chopra confirms YRF will produce Jessica ...   \n",
       "4   Mulayam Yadav to contest 2019 polls from Mainp...   \n",
       "5   I am so excited to play under Virat Kohli's ca...   \n",
       "6   Twitter reacts to women-only screening of Wond...   \n",
       "7   Apple to hire engineer with psychology backgro...   \n",
       "8   Railway Board Chairman resigns after 2 derailm...   \n",
       "9              Who is billionaire Radhakishan Damani?   \n",
       "10  Flyers told to jump onto tarmac after threaten...   \n",
       "11  My mother asked me how could I do this to Hard...   \n",
       "12  4 years since Beckham announced his retirement...   \n",
       "13  We've gone through so many CTOs, it's a joke: ...   \n",
       "14  People in the film industry call me a bull: Ar...   \n",
       "15  Govt proposes airing public interest messages ...   \n",
       "16  Maybe true: 'Taarak Mehta Ka...' producer on D...   \n",
       "17  Which teams have qualified for 2018 FIFA World...   \n",
       "18  Karnataka govt plans 1 lakh houses for poor in...   \n",
       "19  Pak SC bars Nawaz Sharif from holding public o...   \n",
       "\n",
       "                                                 text  \n",
       "0   Digital payments startup Paytm has raised $1.4...  \n",
       "1   Oil companies on Thursday reduced the petrol p...  \n",
       "2   The Indian Army has announced plans to deploy ...  \n",
       "3   Yash Raj Films CEO Uday Chopra has confirmed t...  \n",
       "4   Senior Samajwadi Party leader Ram Gopal Yadav ...  \n",
       "5   Batsman Shubman Gill, who has been included in...  \n",
       "6   Reacting to Texas' The Alamo Drafthouse theatr...  \n",
       "7   Apple is hiring a software engineer with psych...  \n",
       "8   The Chairman of the Railway Board, Ashok Mitta...  \n",
       "9   Radhakishan Damani is the 61-year-old billiona...  \n",
       "10  Passengers were told to \"jump\" onto the tarmac...  \n",
       "11  After Hardik Pandya and KL Rahul were suspende...  \n",
       "12  Former England captain David Beckham announced...  \n",
       "13  When asked about the difficulties faced while ...  \n",
       "14  Actor Arjun Rampal has said that he feels like...  \n",
       "15  The Information and Broadcasting (I&B) Ministr...  \n",
       "16  'Taarak Mehta Ka Ooltah Chashmah' producer Asi...  \n",
       "17  France, who are among the six European teams t...  \n",
       "18  Karnataka Chief Minister Siddaramaiah will soo...  \n",
       "19  The Pakistan Supreme Court on Friday disqualif...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv(\"data/news_summary_train.csv\")\n",
    "print('Train Length: ',len(train_df))\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Length:  14754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Govt forms SIT in Ryan murder case, CBSE seeks...</td>\n",
       "      <td>The HRD Ministry has formed a three-member Spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indrani asks for furniture, jewellery in divor...</td>\n",
       "      <td>In a letter written from jail, Sheena Bora mur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ED raids 35 premises of Nirav Modi, â¹550-cr ...</td>\n",
       "      <td>The Enforcement Directorate (ED) on Friday con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japan admits 1st death from 2011 Fukushima nuc...</td>\n",
       "      <td>Japan has acknowledged for the first time that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An entire village in Germany is being auctione...</td>\n",
       "      <td>An entire village in Germany is being auctione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Man Utd luckiest PL team, Liverpool unluckiest...</td>\n",
       "      <td>Manchester United were the luckiest team while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Raina overtakes Kohli to become IPL's top run-...</td>\n",
       "      <td>CSK's Suresh Raina overtook RCB captain Virat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mumbai housing society saves â¹2 lakh a month...</td>\n",
       "      <td>Residents of a 20-storey housing complex in Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NASA launches two Antarctic flights from two c...</td>\n",
       "      <td>For the first time in its nine years of operat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pentagon slammed for wasting â¹180 cr on Afgh...</td>\n",
       "      <td>US Defence Secretary James Mattis has criticis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Muslims pray outside White House in protest ov...</td>\n",
       "      <td>Hundreds of Muslims prayed outside the White H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Swimming, water sports on Goa beaches closed o...</td>\n",
       "      <td>Swimming and water sports across beaches in Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I disagree with the term 'item song': Katrina ...</td>\n",
       "      <td>Actress Katrina Kaif has said she disagrees wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>You can hide marriage, not pregnancy: Anushka ...</td>\n",
       "      <td>Anushka Sharma, while denying rumours of her p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Will remove your hand from cycle handle: Akhil...</td>\n",
       "      <td>Former Uttar Pradesh CM Akhilesh Yadav has acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finch scores fastest fifty for GL in 7-wicket ...</td>\n",
       "      <td>Gujarat Lions defeated Royal Challengers Banga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mumbai cyclone alert a rumour, clarifies civic...</td>\n",
       "      <td>After messages about a cyclone in Mumbai went ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'Ball of the Century' like delivery bowled in ...</td>\n",
       "      <td>Australian women's team spinner Amanda-Jade We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I'm not a Muslim &amp; I've to be woken up by Azaa...</td>\n",
       "      <td>Singer Sonu Nigam on Monday took to Twitter to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7-year-old girl raped on neighbour's terrace i...</td>\n",
       "      <td>A seven-year-old girl was allegedly raped by a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headlines  \\\n",
       "0   Govt forms SIT in Ryan murder case, CBSE seeks...   \n",
       "1   Indrani asks for furniture, jewellery in divor...   \n",
       "2   ED raids 35 premises of Nirav Modi, â¹550-cr ...   \n",
       "3   Japan admits 1st death from 2011 Fukushima nuc...   \n",
       "4   An entire village in Germany is being auctione...   \n",
       "5   Man Utd luckiest PL team, Liverpool unluckiest...   \n",
       "6   Raina overtakes Kohli to become IPL's top run-...   \n",
       "7   Mumbai housing society saves â¹2 lakh a month...   \n",
       "8   NASA launches two Antarctic flights from two c...   \n",
       "9   Pentagon slammed for wasting â¹180 cr on Afgh...   \n",
       "10  Muslims pray outside White House in protest ov...   \n",
       "11  Swimming, water sports on Goa beaches closed o...   \n",
       "12  I disagree with the term 'item song': Katrina ...   \n",
       "13  You can hide marriage, not pregnancy: Anushka ...   \n",
       "14  Will remove your hand from cycle handle: Akhil...   \n",
       "15  Finch scores fastest fifty for GL in 7-wicket ...   \n",
       "16  Mumbai cyclone alert a rumour, clarifies civic...   \n",
       "17  'Ball of the Century' like delivery bowled in ...   \n",
       "18  I'm not a Muslim & I've to be woken up by Azaa...   \n",
       "19  7-year-old girl raped on neighbour's terrace i...   \n",
       "\n",
       "                                                 text  \n",
       "0   The HRD Ministry has formed a three-member Spe...  \n",
       "1   In a letter written from jail, Sheena Bora mur...  \n",
       "2   The Enforcement Directorate (ED) on Friday con...  \n",
       "3   Japan has acknowledged for the first time that...  \n",
       "4   An entire village in Germany is being auctione...  \n",
       "5   Manchester United were the luckiest team while...  \n",
       "6   CSK's Suresh Raina overtook RCB captain Virat ...  \n",
       "7   Residents of a 20-storey housing complex in Ka...  \n",
       "8   For the first time in its nine years of operat...  \n",
       "9   US Defence Secretary James Mattis has criticis...  \n",
       "10  Hundreds of Muslims prayed outside the White H...  \n",
       "11  Swimming and water sports across beaches in Go...  \n",
       "12  Actress Katrina Kaif has said she disagrees wi...  \n",
       "13  Anushka Sharma, while denying rumours of her p...  \n",
       "14  Former Uttar Pradesh CM Akhilesh Yadav has acc...  \n",
       "15  Gujarat Lions defeated Royal Challengers Banga...  \n",
       "16  After messages about a cyclone in Mumbai went ...  \n",
       "17  Australian women's team spinner Amanda-Jade We...  \n",
       "18  Singer Sonu Nigam on Monday took to Twitter to...  \n",
       "19  A seven-year-old girl was allegedly raped by a...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df=pd.read_csv(\"data/news_summary_valid.csv\")\n",
    "print('Valid Length: ',len(valid_df))\n",
    "valid_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Digital payments startup Paytm has raised $1.4 billion from SoftBank in India's largest funding round. This is also SoftBank's biggest investment in the Indian startup ecosystem till date. The latest investment by SoftBank will value Paytm at around $8 billion, up from its valuation of $4.8 billion in August 2016. \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
